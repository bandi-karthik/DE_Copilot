import os
import re
import boto3
import time
import json, copy, urllib3

# boto3 clients
http = urllib3.PoolManager()
region = 'us-east-1'
s3_client = boto3.client('s3')
glue_client = boto3.client('glue', region_name=region)
athena_client = boto3.client('athena',region_name=region)

# config
default_pii = ["name", "email", "phone_number", "salary","ssn","credit_card"]
s3_bucket = 'de-copilot-s3'
athena_output = f"s3://{s3_bucket}/athena-results/"




# metadeta stats of the tables in the glue
def glue_metadata(database, table, region):
    try:
        resp = glue_client.get_table(DatabaseName=database, Name=table)
        t = resp['Table']
        user_defined_pii = []
        schema = []
        for col in t["StorageDescriptor"]["Columns"]:
            is_pk = False
            if 'pk' in (col.get('Comment','')).lower() or 'primary_key' in (col.get('Comment', '')).lower():
                is_pk = True

            is_fk = False
            if 'fk' in (col.get('Comment', '')).lower() or 'foreign_key' in (col.get('Comment', '')).lower():
                is_fk = True    

            if 'pii_column' in (col.get('Comment', '')).lower():
                user_defined_pii.append(col["Name"])
                

            schema.append({
                "name": col["Name"],
                "type": col["Type"],
                "nullable": False if is_pk else True,
                "partition_key": False,
                "primary_key" : is_pk,
                "foregin_key" : is_fk, 
                "comments" : col.get("Comment", "")
            })
  
        for col in t.get("PartitionKeys", []):
            schema.append({
                "name": col["Name"],
                "type": col["Type"],
                "nullable": True,
                "partition_key": True,
                "primary_key" : False,
                "foreign_key" : False,
                "comments" : col.get("Comment", "")  
            })
 
        return ({
            "table_name": t["Name"],
            "database": t["DatabaseName"],
            "schema": schema
        },user_defined_pii)

    except Exception as e:
        print("error in glue_metadata:", str(e))
        return ({}, [])

def build_stats_sql(database, table, schema, pii_list):

    pii_columns = set(c.lower() for c in pii_list)
    selects = []

    row_cnts = f""" 
                SELECT
                'ROW_COUNT' as col_name,
                CAST(COUNT(*) AS VARCHAR) AS min_val,
                NULL as max_val,
                NULL as null_pct,
                NULL as distinct_count
                FROM "{database}"."{table}"
                """.strip()
    selects.append(row_cnts)

    for col in schema:
        col_name = col["name"]
        col_lower = col_name.lower()

        if col_lower in pii_columns:
            continue

        s = f"""
            SELECT
            '{col_name}' AS col_name,
            CAST(MIN("{col_name}") AS VARCHAR) AS min_val,
            CAST(MAX("{col_name}") AS VARCHAR) AS max_val,
            AVG(CASE WHEN "{col_name}" IS NULL THEN 1.0 ELSE 0 END) AS null_pct,
            APPROX_DISTINCT("{col_name}") AS distinct_count
            FROM "{database}"."{table}"
            """.strip()

        selects.append(s)

    if not selects:
        return None

    sql = "\nUNION ALL\n".join(selects)
    return sql


def athena_setup(database,region,query,athena_output):
    try:
        response = athena_client.start_query_execution(QueryString=query, QueryExecutionContext = {'Database' : database},
        ResultConfiguration = {'OutputLocation':athena_output})

        id = response['QueryExecutionId'] # generated by athena as a ticket number
        
        while True:
            stats = athena_client.get_query_execution(QueryExecutionId=id)
            status = stats['QueryExecution']['Status']['State']

            if status =='SUCCEEDED':
                break 
            elif status in ['FAILED', 'CANCELLED']:
                reason = stats['QueryExecution']['Status'].get('StateChangeReason', 'Unknown')
                raise Exception(f"Query Failed: {reason}")

            time.sleep(5)

        return athena_client.get_query_results(QueryExecutionId=id)

    except Exception as e:
        print("error in run_athena_query:", str(e))
        return None 

# get the column value stats, runs on athena
def get_athena_data(database, table, region, schema, pii_list, athena_output):
    try:
        sql = build_stats_sql(database, table, schema, pii_list)

        if not sql:
            print("no non-PII columns found for stats")
            return {}

        query_out = athena_setup(database, region, sql, athena_output)

        if not query_out:
            return {}

        rows = query_out["ResultSet"]["Rows"]
        col_stats = parse_stats_rows(rows)


        return col_stats

    except Exception as e:
        print("error in get_athena_data:", str(e))
        return {}

def parse_stats_rows(rows):
    data_rows = rows[1:]

    stats = {'ROW_COUNT' : 0}

    for r in data_rows:
        vals = [c.get("VarCharValue") for c in r["Data"]]


        col_name = vals[0]
        min_val = vals[1]
        if col_name == 'ROW_COUNT':
            stats['ROW_COUNT'] = min_val if min_val else 0
            continue

        max_val = vals[2]
        null_pct = vals[3]
        distinct = vals[4]

        try:
            null_pct = float(null_pct) if null_pct is not None else None
        except:
            pass

        try:
            distinct = int(distinct) if distinct is not None else None
        except:
            pass

        stats[col_name] = {
            "min": min_val,
            "max": max_val,
            "null_pct": null_pct,
            "distinct_count": distinct
        }

    return stats

    

# remove the pii information in the column stats from athena
def filter_pii(col_stats, pii_list):
    pii_columns = set(c.lower() for c in pii_list)

    cleaned_stats = {}
    for col_name, vals in col_stats.items():
        if col_name.lower() not in pii_columns:
            cleaned_stats[col_name] = vals

    return cleaned_stats

def build_prompt(cur_ddl):
    prompt = s3_client.get_object(Bucket = 'de-copilot-s3',Key='prompt/llm_prompt.txt')
    prompt = prompt['Body'].read()
    prompt = prompt.decode('utf-8')
           
    glue_ddl = json.dumps(cur_ddl,indent = 2)
    
    final_prompt = prompt.replace("{ddl_obj}", glue_ddl)
    
    return final_prompt

def call_gemini(payload,table):
    try:
        api_key = os.environ['GEMINI_API_KEY']
        url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?key={api_key}"
        
        headers = {"Content-Type": "application/json"}
        data = {"contents": [{"parts": [{"text": payload}]}]}
        
        encoded_data = json.dumps(data).encode("utf-8")
        
        resp = http.request("POST", url, body=encoded_data, headers=headers)
        

        resp_json = json.loads(resp.data.decode("utf-8"))
        text = resp_json["candidates"][0]["content"]["parts"][0]["text"]
        clean_json_str = re.sub(r"```json\n?|```", "", text).strip()
        cleaned_json = json.loads(clean_json_str)
        
        s3_client.put_object(Bucket='de-copilot-s3',Key=f'contracts/{table}.json',Body=json.dumps(cleaned_json, indent=2),ContentType='application/json')

    except Exception as e:
        print("error in call_gemini:", str(e))

def lambda_handler(event, context):
    print("DE_COPILOT_GEN_CONTRACTS Triggered!")

    target_db = event.get('database')
    target_table = event.get('table')

    if 'detail' in event:
        detail = event['detail']
        target_db = detail.get('databaseName', target_db)

        if 'changedTables' in detail:
            changed = detail.get('changedTables', [])
            if changed:
                target_table = changed[0]

        if 'tableName' in detail:
            target_table = detail.get('tableName', target_table)

    if not target_db or not target_table:
        print("No database or table provided")
        return {"statusCode": 400, "body": "No database or table provided"}

    ddl_obj, user_defined_pii = glue_metadata(target_db, target_table, region)
    if not ddl_obj:
        return {"statusCode": 400, "body": f"No DDL created for {target_db}.{target_table}"}

    if not user_defined_pii:
        user_defined_pii = default_pii

    schema = ddl_obj.get("schema", [])
    col_stats = get_athena_data(target_db, target_table, region, schema, user_defined_pii, athena_output)

    clean_stats = filter_pii(col_stats, user_defined_pii)

    row_count = clean_stats.get("ROW_COUNT", 0)
    column_stats = {k: v for k, v in clean_stats.items() if k != "ROW_COUNT"}

    ddl_obj["row_count"] = row_count
    ddl_obj["column_stats"] = column_stats

    print(f'Building the prompt! for {target_db}.{target_table}')
    payload = build_prompt(ddl_obj)
    print(f'Creating the Contracts for {target_db}.{target_table}')
    call_gemini(payload, target_table)
    print(f'Contracts created for {target_db}.{target_table} at s3://de-copilot-s3/contracts/{target_table}.json')
    return {
        "statusCode": 200,
        "body": f"Processed {target_db}.{target_table},Contracts stored at s3://de-copilot-s3/contracts/{target_table}.json "
    }

