{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55d8b982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import boto3\n",
    "import time\n",
    "import json, copy\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "300ffd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = 'copilot_demo'\n",
    "table = 'employees_test'\n",
    "region = 'us-east-1'\n",
    "USER_DEFINED_PII = [\"name\", \"email\", \"phone_number\", \"salary\"]\n",
    "s3_bucket = \"s3://de-copilot-s3/athena-results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1fe4858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"table_name\": \"employees_test\",\n",
      "  \"database\": \"copilot_demo\",\n",
      "  \"schema\": [\n",
      "    {\n",
      "      \"name\": \"emp_id\",\n",
      "      \"type\": \"int\",\n",
      "      \"nullable\": false,\n",
      "      \"partition_key\": false,\n",
      "      \"primary_key\": true,\n",
      "      \"foregin_key\": false,\n",
      "      \"comments\": \"primary_key\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"name\",\n",
      "      \"type\": \"string\",\n",
      "      \"nullable\": true,\n",
      "      \"partition_key\": false,\n",
      "      \"primary_key\": false,\n",
      "      \"foregin_key\": false,\n",
      "      \"comments\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"salary\",\n",
      "      \"type\": \"double\",\n",
      "      \"nullable\": true,\n",
      "      \"partition_key\": false,\n",
      "      \"primary_key\": false,\n",
      "      \"foregin_key\": false,\n",
      "      \"comments\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"department\",\n",
      "      \"type\": \"string\",\n",
      "      \"nullable\": true,\n",
      "      \"partition_key\": false,\n",
      "      \"primary_key\": false,\n",
      "      \"foregin_key\": false,\n",
      "      \"comments\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"joining_date\",\n",
      "      \"type\": \"date\",\n",
      "      \"nullable\": true,\n",
      "      \"partition_key\": false,\n",
      "      \"primary_key\": false,\n",
      "      \"foregin_key\": false,\n",
      "      \"comments\": \"\"\n",
      "    }\n",
      "  ],\n",
      "  \"column_stats\": {\n",
      "    \"ROW_COUNT\": \"0\",\n",
      "    \"department\": {\n",
      "      \"min\": null,\n",
      "      \"max\": null,\n",
      "      \"null_pct\": null,\n",
      "      \"distinct_count\": 0\n",
      "    },\n",
      "    \"emp_id\": {\n",
      "      \"min\": null,\n",
      "      \"max\": null,\n",
      "      \"null_pct\": null,\n",
      "      \"distinct_count\": 0\n",
      "    },\n",
      "    \"joining_date\": {\n",
      "      \"min\": null,\n",
      "      \"max\": null,\n",
      "      \"null_pct\": null,\n",
      "      \"distinct_count\": 0\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# metadeta stats of the tables in the glue\n",
    "def glue_metadata(database, table, region):\n",
    "    try:\n",
    "        client = boto3.client('glue', region_name=region)\n",
    "        resp = client.get_table(DatabaseName=database, Name=table)\n",
    "        t = resp['Table']\n",
    "\n",
    "        schema = []\n",
    "        for col in t[\"StorageDescriptor\"][\"Columns\"]:\n",
    "            is_pk = False\n",
    "            if (col['Comment']).lower() in ['pk','primary_key']:\n",
    "                is_pk = True\n",
    "            is_fk = False\n",
    "            if (col['Comment']).lower() in ['fk','foreign_key']:\n",
    "                is_fk = True\n",
    "\n",
    "            schema.append({\n",
    "                \"name\": col[\"Name\"],\n",
    "                \"type\": col[\"Type\"],\n",
    "                \"nullable\": False if is_pk else True,\n",
    "                \"partition_key\": False,\n",
    "                \"primary_key\" : is_pk,\n",
    "                \"foregin_key\" : is_fk, \n",
    "                \"comments\" : col.get(\"Comment\", \"\")\n",
    "            })\n",
    "  \n",
    "        for col in t.get(\"PartitionKeys\", []):\n",
    "            schema.append({\n",
    "                \"name\": col[\"Name\"],\n",
    "                \"type\": col[\"Type\"],\n",
    "                \"nullable\": True,\n",
    "                \"partition_key\": True,\n",
    "                \"primary_key\" : False,\n",
    "                \"foreign_key\" : False,\n",
    "                \"comments\" : col.get(\"Comment\", \"\")  \n",
    "            })\n",
    " \n",
    "        return {\n",
    "            \"table_name\": t[\"Name\"],\n",
    "            \"database\": t[\"DatabaseName\"],\n",
    "            \"schema\": schema\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"error in glue_metadata:\", str(e))\n",
    "        return {}\n",
    "\n",
    "def build_stats_sql(database, table, schema, pii_list):\n",
    "\n",
    "    pii_columns = set(c.lower() for c in pii_list)\n",
    "    selects = []\n",
    "\n",
    "    row_cnts = f\"\"\" \n",
    "                SELECT\n",
    "                'ROW_COUNT' as col_name,\n",
    "                CAST(COUNT(*) AS VARCHAR) AS min_val,\n",
    "                NULL as max_val,\n",
    "                NULL as null_pct,\n",
    "                NULL as distinct_count\n",
    "                FROM \"{database}\".\"{table}\"\n",
    "                \"\"\".strip()\n",
    "    selects.append(row_cnts)\n",
    "\n",
    "    for col in schema:\n",
    "        col_name = col[\"name\"]\n",
    "        col_lower = col_name.lower()\n",
    "\n",
    "        if col_lower in pii_columns:\n",
    "            continue\n",
    "\n",
    "        s = f\"\"\"\n",
    "            SELECT\n",
    "            '{col_name}' AS col_name,\n",
    "            CAST(MIN(\"{col_name}\") AS VARCHAR) AS min_val,\n",
    "            CAST(MAX(\"{col_name}\") AS VARCHAR) AS max_val,\n",
    "            AVG(CASE WHEN \"{col_name}\" IS NULL THEN 1.0 ELSE 0 END) AS null_pct,\n",
    "            APPROX_DISTINCT(\"{col_name}\") AS distinct_count\n",
    "            FROM \"{database}\".\"{table}\"\n",
    "            \"\"\".strip()\n",
    "\n",
    "        selects.append(s)\n",
    "\n",
    "    if not selects:\n",
    "        return None\n",
    "\n",
    "    sql = \"\\nUNION ALL\\n\".join(selects)\n",
    "    return sql\n",
    "\n",
    "\n",
    "def athena_setup(database,region,query,s3_bucket):\n",
    "    try:\n",
    "        athena_client = boto3.client('athena',region_name=region)\n",
    "\n",
    "        response = athena_client.start_query_execution(QueryString=query, QueryExecutionContext = {'Database' : database},\n",
    "        ResultConfiguration = {'OutputLocation':s3_bucket})\n",
    "\n",
    "        id = response['QueryExecutionId'] # generated by athena as a ticket number\n",
    "        \n",
    "        while True:\n",
    "            stats = athena_client.get_query_execution(QueryExecutionId=id)\n",
    "            status = stats['QueryExecution']['Status']['State']\n",
    "\n",
    "            if status =='SUCCEEDED':\n",
    "                break \n",
    "            elif status in ['FAILED', 'CANCELLED']:\n",
    "                reason = stats['QueryExecution']['Status'].get('StateChangeReason', 'Unknown')\n",
    "                raise Exception(f\"Query Failed: {reason}\")\n",
    "\n",
    "            time.sleep(5)\n",
    "\n",
    "        return athena_client.get_query_results(QueryExecutionId=id)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"error in run_athena_query:\", str(e))\n",
    "        return None \n",
    "\n",
    "# get the column value stats, runs on athena\n",
    "def get_athena_data(database, table, region, schema, pii_list, s3_bucket):\n",
    "    try:\n",
    "        sql = build_stats_sql(database, table, schema, pii_list)\n",
    "\n",
    "        if not sql:\n",
    "            print(\"no non-PII columns found for stats\")\n",
    "            return {}\n",
    "\n",
    "        query_out = athena_setup(database, region, sql, s3_bucket)\n",
    "\n",
    "        if not query_out:\n",
    "            return {}\n",
    "\n",
    "        rows = query_out[\"ResultSet\"][\"Rows\"]\n",
    "        col_stats = parse_stats_rows(rows)\n",
    "\n",
    "\n",
    "        return col_stats\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"error in get_athena_data:\", str(e))\n",
    "        return {}\n",
    "\n",
    "def parse_stats_rows(rows):\n",
    "    data_rows = rows[1:]\n",
    "\n",
    "    stats = {'ROW_COUNT' : 0}\n",
    "\n",
    "    for r in data_rows:\n",
    "        vals = [c.get(\"VarCharValue\") for c in r[\"Data\"]]\n",
    "\n",
    "\n",
    "        col_name = vals[0]\n",
    "        min_val = vals[1]\n",
    "        if col_name == 'ROW_COUNT':\n",
    "            stats['ROW_COUNT'] = min_val if min_val else 0\n",
    "            continue\n",
    "\n",
    "        max_val = vals[2]\n",
    "        null_pct = vals[3]\n",
    "        distinct = vals[4]\n",
    "\n",
    "        try:\n",
    "            null_pct = float(null_pct) if null_pct is not None else None\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            distinct = int(distinct) if distinct is not None else None\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        stats[col_name] = {\n",
    "            \"min\": min_val,\n",
    "            \"max\": max_val,\n",
    "            \"null_pct\": null_pct,\n",
    "            \"distinct_count\": distinct\n",
    "        }\n",
    "\n",
    "    return stats\n",
    "\n",
    "    \n",
    "\n",
    "# remove the pii information in the column stats from athena\n",
    "def filter_pii(col_stats, pii_list):\n",
    "    pii_columns = set(c.lower() for c in pii_list)\n",
    "\n",
    "    cleaned_stats = {}\n",
    "    for col_name, vals in col_stats.items():\n",
    "        if col_name.lower() not in pii_columns:\n",
    "            cleaned_stats[col_name] = vals\n",
    "\n",
    "    return cleaned_stats\n",
    "\n",
    "\n",
    "\n",
    "ddl_obj = glue_metadata(database, table, region)\n",
    "schema = ddl_obj.get(\"schema\", [])\n",
    "\n",
    "col_stats = get_athena_data(database, table, region, schema, USER_DEFINED_PII, s3_bucket)\n",
    "\n",
    "\n",
    "col_stats = filter_pii(col_stats, USER_DEFINED_PII)\n",
    "\n",
    "ddl_obj[\"column_stats\"] = col_stats\n",
    "\n",
    "print(json.dumps(ddl_obj, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91604a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = f\"\"\"\n",
    "Pay Attention, Do not hallucinate, only work on what is there in the below and think deep for all the edge cases for the below requirements.\n",
    "Do NOT assume any properties (file format, update frequency, row counts, S3 location, etc.) that are not explicitly present in the JSON. If unknown, either omit or mark as \"unknown\".\n",
    "\n",
    "You are a Senior Data Engineer Copilot specializing in **AWS Glue and Athena**.\n",
    "\n",
    "You will receive a schema object extracted directly from the **AWS Glue Data Catalog**.\n",
    "Your goal is to generate a Data Quality Contract and Post-Load Tests suitable for an AWS Data Lake environment and also confulence style documentation for the table.\n",
    "\n",
    "The JSON object will look like this (shape, not exact values):\n",
    "\n",
    "{json.dumps(ddl_obj, indent=2)}\n",
    "\n",
    "Where:\n",
    "- table_name: name of the table\n",
    "- database: database / schema name\n",
    "- schema: list of columns, each like:\n",
    "  - name: column name\n",
    "  - type: data type (string, int, double, date, etc.)\n",
    "  - nullable: true / false\n",
    "- column_stats: ONLY for NON-PII columns, something like:\n",
    "  - min, max, null_pct, distinct_count, top_values\n",
    "- constraints: optional list of primary keys, unique keys, check constraints\n",
    "- job_summary: includes inputs, filters, and grain if available\n",
    "- rule_type MUST be exactly one of the allowed values. \n",
    "  Do NOT invent new rule_type names. If unsure, choose the closest one.\n",
    "\n",
    "IMPORTANT PRIVACY RULES:\n",
    "- PII columns (name, email, phone_number, salary, etc.) appear in \"schema\"\n",
    "  but their stats and values are NOT provided.\n",
    "- For PII columns:\n",
    "  - You MAY define structural rules: not_null, not_empty, length, regex.\n",
    "  - You MUST NOT include any concrete example values (no fake SSNs, emails, phones).\n",
    "  - Only describe patterns, like \"must be 9 digits\", \"must match email format\".\n",
    "- For non-PII columns:\n",
    "  - You MAY use column_stats to propose ranges and allowed_values.\n",
    "  - Still avoid writing specific sample values in descriptions; talk about rules.\n",
    "\n",
    "----------------------------------------\n",
    "THINKING / COVERAGE REQUIREMENTS\n",
    "----------------------------------------\n",
    "\n",
    "You must think column-by-column and constraint-by-constraint. \n",
    "Do not skip any column.\n",
    "For coverage:\n",
    "- Every column in \"schema\" (except purely technical partition columns) MUST appear in at least one rule in data_quality.rules.\n",
    "- Do NOT skip columns just because nullable = true. If a column is nullable, you can still enforce rules like \"if present, must not be empty\" or \"if present, must match pattern\".\n",
    "\n",
    "For every column in \"schema\":\n",
    "\n",
    "1. COMPLETENESS\n",
    "   IMPORTANT:\n",
    "    - If nullable = true, you MUST NOT create a not_null rule.\n",
    "    - Nullable columns must allow NULL in all spark_exp expressions.\n",
    "    - Never generate contradictory rules for any column. If nullable = true:\n",
    "        * Do NOT generate not_null\n",
    "        * Do NOT generate conditions that fail for NULL\n",
    "\n",
    "   - If nullable = false → always generate a not_null rule.\n",
    "   - For ALL string-like columns (string, varchar, char), regardless of nullable:\n",
    "       * Always generate a not_empty-style rule:\n",
    "         - If nullable = true: use logic \"value IS NULL OR trimmed length > 0\".\n",
    "         - If nullable = false: use logic \"value IS NOT NULL AND trimmed length > 0\".\n",
    "\n",
    "2. VALIDITY\n",
    "   Use the combination of:\n",
    "   - column name\n",
    "   - data type \n",
    "   - column_stats (only non-PII)\n",
    "   - constraints (CHECK, PK, UNIQUE)\n",
    "   to infer validity rules such as:\n",
    "     * numeric columns >= 0 unless obviously not applicable\n",
    "     * string columns with stable lengths → infer min_length / max_length or regex\n",
    "     * year/date columns must not be in the future\n",
    "     * codes (country_code, dep_code) must be in allowed_values if low-cardinality\n",
    "\n",
    "3. RANGE RULES (NON-PII ONLY)\n",
    "   Use column_stats[min_val, max_val, distinct_count, null_pct].\n",
    "   Create soft WARNING rules with a 20–25% buffer around min/max or p95 if present.\n",
    "\n",
    "4. ALLOWED VALUES (NON-PII ONLY)\n",
    "   If distinct_count is small (< 50) AND stable → generate allowed_values.\n",
    "\n",
    "5. PII COLUMNS\n",
    "   - PII columns appear in schema but have NO column_stats.\n",
    "   - For these columns you MUST generate:\n",
    "       * not_null (if nullable = false)\n",
    "       * not_empty for strings\n",
    "       * regex or fixed-length patterns inferred ONLY from schema + column name\n",
    "   - NEVER include example email or SSN values. Only describe patterns.\n",
    "\n",
    "6. CROSS-COLUMN LOGIC (IF OBVIOUS)\n",
    "   If year/date columns exist → ensure year <= current year.\n",
    "   If ID + email exist → email should not be null if ID exists.\n",
    "   If joining_date and resign_date exist → resign_date >= joining_date.\n",
    "\n",
    "7. TABLE-LEVEL RULES\n",
    "   - If constraints include primary key → include uniqueness rule.\n",
    "   - Add table-level rule: row_count > 0.\n",
    "\n",
    "8. Data Quality (pre-load PySpark)\n",
    "   For each rule, you MUST output \"spark_exp\" using **Spark SQL syntax only**, not PySpark API.\n",
    "    spark_exp MUST be a SQL expression that can be passed directly into:\n",
    "\n",
    "    df.filter(expr(spark_exp))\n",
    "    Examples of valid spark_exp:\n",
    "      \"salary >= 0\"\n",
    "      \"salary IS NULL OR salary >= 0\"\n",
    "      \"name IS NULL OR length(trim(name)) > 0\"\n",
    "      \"joining_date <= current_date()\"\n",
    "\n",
    "    Examples of INVALID spark_exp (do NOT generate these):\n",
    "      col('salary') >= 0\n",
    "      F.col(\"name\").isNull()\n",
    "      dataframe.count() > 0\n",
    "      salary.notNull()\n",
    "\n",
    "9. TEST COVERAGE (Post-load SQL)\n",
    "   You must generate SQL tests for:\n",
    "       * uniqueness of PK/grain\n",
    "       * null checks on required columns\n",
    "       * each CHECK constraint\n",
    "       * future-date violations\n",
    "       * allowed_values validation (for low-cardinality columns)\n",
    "       * numeric range violations\n",
    "\n",
    "After generating rules and tests, REVIEW:\n",
    "- Did you include ALL columns?\n",
    "- Did you cover ALL non-nullable columns?\n",
    "- Did you enforce ALL constraints?\n",
    "- Did you create BOTH rules AND tests?\n",
    "\n",
    "\n",
    "----------------------------------------\n",
    "OUTPUT FORMAT (MUST BE VALID JSON)\n",
    "----------------------------------------\n",
    "Important: In the final JSON, the set of column names used in data_quality.rules (excluding \"__TABLE__\") MUST match the set of column names in \"schema\" (case-insensitive). Do not omit any columns.\n",
    "           Coverage requirement does NOT override the schema.\n",
    "           If a column is nullable, you may generate \"if present, must...\" rules\n",
    "           (e.g., not_empty, min/max with NULL allowed), \n",
    "           but you MUST NOT force mandatory constraints such as not_null.\n",
    "\n",
    "Return ONLY valid JSON in this exact structure (no extra comments):\n",
    "\n",
    "{{\n",
    "  \"data_quality\": {{\n",
    "    \"rules\": [\n",
    "      {{\n",
    "        \"column\": \"col_name_or__TABLE__for_table_level\",\n",
    "        \"rule_type\": \"not_null | not_empty | min | max | allowed_values | regex | pk | fk | check_constraint | custom_sql\",\n",
    "        \"condition\": \"value / list / SQL expression / description string\",\n",
    "        \"severity\": \"ERROR | WARNING\",\n",
    "        \"action\": \"FAIL_JOB | DROP_ROW | WARN\",\n",
    "        \"description\": \"Short reasoning for the rule (no concrete example values).\",\n",
    "        \"spark_exp\": \"A PySpark boolean expression that returns TRUE for valid rows and can be passed directly to pyspark.sql.functions.expr(). \n",
    "                      It MUST NOT reference any DataFrame variable and MUST NOT call actions like count(), groupBy(), collect(), etc. \n",
    "                      Examples: \\\"col('salary') >= 0\\\", \\\"(col('name').isNull()) OR (length(trim(col('name'))) > 0)\\\".\"\n",
    "\n",
    "      }}\n",
    "    ]\n",
    "  }},\n",
    "  \"tests\": [\n",
    "    {{\n",
    "      \"name\": \"test_name\",\n",
    "      \"sql\": \"SELECT ...\",\n",
    "      \"description\": \"What this test validates.\"\n",
    "    }}\n",
    "  ],\n",
    "  \"docs_markdown\": \"# Table Documentation\\\\n...\"\n",
    "}}\n",
    "\n",
    "Do NOT include anything outside this JSON object.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6384c1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"data_quality\": {\n",
      "    \"rules\": [\n",
      "      {\n",
      "        \"column\": \"emp_id\",\n",
      "        \"rule_type\": \"not_null\",\n",
      "        \"condition\": \"must not be null\",\n",
      "        \"severity\": \"ERROR\",\n",
      "        \"action\": \"FAIL_JOB\",\n",
      "        \"description\": \"Employee ID is a required field and serves as the primary key.\",\n",
      "        \"spark_exp\": \"emp_id IS NOT NULL\"\n",
      "      },\n",
      "      {\n",
      "        \"column\": \"emp_id\",\n",
      "        \"rule_type\": \"pk\",\n",
      "        \"condition\": \"primary_key\",\n",
      "        \"severity\": \"ERROR\",\n",
      "        \"action\": \"FAIL_JOB\",\n",
      "        \"description\": \"Employee ID must be unique across all records.\",\n",
      "        \"spark_exp\": \"true\"\n",
      "      },\n",
      "      {\n",
      "        \"column\": \"emp_id\",\n",
      "        \"rule_type\": \"min\",\n",
      "        \"condition\": \"0\",\n",
      "        \"severity\": \"ERROR\",\n",
      "        \"action\": \"DROP_ROW\",\n",
      "        \"description\": \"Employee ID must be a positive integer.\",\n",
      "        \"spark_exp\": \"emp_id > 0\"\n",
      "      },\n",
      "      {\n",
      "        \"column\": \"name\",\n",
      "        \"rule_type\": \"not_empty\",\n",
      "        \"condition\": \"if present, must not be empty or whitespace\",\n",
      "        \"severity\": \"WARNING\",\n",
      "        \"action\": \"WARN\",\n",
      "        \"description\": \"Employee name, if provided, must contain non-whitespace characters.\",\n",
      "        \"spark_exp\": \"name IS NULL OR length(trim(name)) > 0\"\n",
      "      },\n",
      "      {\n",
      "        \"column\": \"salary\",\n",
      "        \"rule_type\": \"min\",\n",
      "        \"condition\": \"0\",\n",
      "        \"severity\": \"WARNING\",\n",
      "        \"action\": \"WARN\",\n",
      "        \"description\": \"Salary, if provided, must be a non-negative value.\",\n",
      "        \"spark_exp\": \"salary IS NULL OR salary >= 0\"\n",
      "      },\n",
      "      {\n",
      "        \"column\": \"department\",\n",
      "        \"rule_type\": \"not_empty\",\n",
      "        \"condition\": \"if present, must not be empty or whitespace\",\n",
      "        \"severity\": \"WARNING\",\n",
      "        \"action\": \"WARN\",\n",
      "        \"description\": \"Department name, if provided, must contain non-whitespace characters.\",\n",
      "        \"spark_exp\": \"department IS NULL OR length(trim(department)) > 0\"\n",
      "      },\n",
      "      {\n",
      "        \"column\": \"joining_date\",\n",
      "        \"rule_type\": \"custom_sql\",\n",
      "        \"condition\": \"<= current_date()\",\n",
      "        \"severity\": \"ERROR\",\n",
      "        \"action\": \"DROP_ROW\",\n",
      "        \"description\": \"The joining date cannot be in the future.\",\n",
      "        \"spark_exp\": \"joining_date IS NULL OR joining_date <= current_date()\"\n",
      "      },\n",
      "      {\n",
      "        \"column\": \"__TABLE__\",\n",
      "        \"rule_type\": \"min\",\n",
      "        \"condition\": \"1\",\n",
      "        \"severity\": \"ERROR\",\n",
      "        \"action\": \"FAIL_JOB\",\n",
      "        \"description\": \"Table must contain at least one row after load.\",\n",
      "        \"spark_exp\": \"true\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"tests\": [\n",
      "    {\n",
      "      \"name\": \"test_pk_uniqueness_emp_id\",\n",
      "      \"sql\": \"SELECT emp_id, COUNT(*) AS duplicate_count FROM copilot_demo.employees_test WHERE emp_id IS NOT NULL GROUP BY emp_id HAVING COUNT(*) > 1\",\n",
      "      \"description\": \"Ensures that emp_id is unique across all rows in the table.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"test_pk_not_null_emp_id\",\n",
      "      \"sql\": \"SELECT COUNT(*) AS null_count FROM copilot_demo.employees_test WHERE emp_id IS NULL\",\n",
      "      \"description\": \"Ensures that the primary key column emp_id contains no null values.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"test_future_joining_date\",\n",
      "      \"sql\": \"SELECT emp_id, joining_date FROM copilot_demo.employees_test WHERE joining_date > current_date\",\n",
      "      \"description\": \"Checks for any records where the joining_date is in the future.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"test_negative_salary\",\n",
      "      \"sql\": \"SELECT emp_id, salary FROM copilot_demo.employees_test WHERE salary < 0\",\n",
      "      \"description\": \"Checks for records with a negative salary value.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"test_empty_strings\",\n",
      "      \"sql\": \"SELECT emp_id FROM copilot_demo.employees_test WHERE (name IS NOT NULL AND trim(name) = '') OR (department IS NOT NULL AND trim(department) = '')\",\n",
      "      \"description\": \"Checks for non-null but empty or whitespace-only strings in key text fields.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"test_table_not_empty\",\n",
      "      \"sql\": \"SELECT 'Table is empty' AS failure_reason WHERE (SELECT COUNT(*) FROM copilot_demo.employees_test) = 0\",\n",
      "      \"description\": \"Checks that the table is not empty after the load process. Fails if row count is 0.\"\n",
      "    }\n",
      "  ],\n",
      "  \"docs_markdown\": \"# Table: employees_test\\n\\n| Property      | Value                 |\\n|---------------|-----------------------|\\n| Database      | `copilot_demo`        |\\n| Table Name    | `employees_test`      |\\n| Data Owner    | unknown               |\\n| Contact       | unknown               |\\n\\n## Description\\nThis table contains employee information, including their ID, name, salary, department, and joining date.\\n\\n## Column Definitions\\n| Column Name    | Data Type | Nullable | Primary Key | Description      |\\n|----------------|-----------|----------|-------------|------------------|\\n| `emp_id`         | int       | false    | **Yes**     | Unique identifier for each employee. This is the primary key. |\\n| `name`           | string    | true     | No          | Employee's full name. This is a PII column. |\\n| `salary`         | double    | true     | No          | Employee's salary. This is a PII column. |\\n| `department`     | string    | true     | No          | The department the employee belongs to. |\\n| `joining_date`   | date      | true     | No          | The date the employee joined the company. |\\n\\n## Data Quality & Post-Load Tests\\nThis table is subject to the following data quality checks and post-load validation tests:\\n*   **Row Count**: The table must not be empty after a successful load.\\n*   **`emp_id`**: Must be a unique, non-null, positive integer. It serves as the primary key.\\n*   **`name`**: If provided, must be a non-empty string.\\n*   **`salary`**: If provided, must be a non-negative number.\\n*   **`department`**: If provided, must be a non-empty string.\\n*   **`joining_date`**: If provided, must not be a date in the future.\\n\\n## Example Query\\n```sql\\nSELECT\\n    emp_id,\\n    name,\\n    salary,\\n    department,\\n    joining_date\\nFROM copilot_demo.employees_test\\nLIMIT 10;\\n```\\n\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "client = genai.Client()\n",
    "response = client.models.generate_content(model='gemini-2.5-pro', contents=payload)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602e6ddc",
   "metadata": {},
   "source": [
    "# Push to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5624404",
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_json_path = \"s3://de-copilot-s3/contracts/\"\n",
    "s3_client = boto3.client('s3',region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6e7abda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_json_str = re.sub(r\"```json\\n|\\n```\", \"\", response.text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e5b1ab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_json = json.loads(clean_json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "32141ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '0FVC1RT15N5N1XCK',\n",
       "  'HostId': 'K0z10dwJhJ4cRdAzTmfpP5r2rfu+dcKHd3leppuDN9vc42FXWify2x3rkkaescimpB5dCXRHL2WTm9pHmmOozw==',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'K0z10dwJhJ4cRdAzTmfpP5r2rfu+dcKHd3leppuDN9vc42FXWify2x3rkkaescimpB5dCXRHL2WTm9pHmmOozw==',\n",
       "   'x-amz-request-id': '0FVC1RT15N5N1XCK',\n",
       "   'date': 'Sat, 22 Nov 2025 00:17:15 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"7486daece0e956f31450cf2f6c9ddc63\"',\n",
       "   'x-amz-checksum-crc32': 'W5rfaQ==',\n",
       "   'x-amz-checksum-type': 'FULL_OBJECT',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"7486daece0e956f31450cf2f6c9ddc63\"',\n",
       " 'ChecksumCRC32': 'W5rfaQ==',\n",
       " 'ChecksumType': 'FULL_OBJECT',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_client.put_object(Bucket='de-copilot-s3',Key='contracts/employees_test.json',Body=json.dumps(cleaned_json,indent=2),ContentType='application/json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c9d63b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
