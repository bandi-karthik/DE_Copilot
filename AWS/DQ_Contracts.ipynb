{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55d8b982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import boto3\n",
    "import time\n",
    "import json, copy\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "300ffd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = 'copilot_demo'\n",
    "table = 'employees_test'\n",
    "region = 'us-east-1'\n",
    "USER_DEFINED_PII = [\"name\", \"email\", \"phone_number\", \"salary\"]\n",
    "s3_bucket = \"s3://de-copilot-s3/athena-results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b88cecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client = boto3.client('glue', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fe4858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadeta stats of the tables in the glue\n",
    "def glue_metadata(database, table, region):\n",
    "    try:\n",
    "        resp = glue_client.get_table(DatabaseName=database, Name=table)\n",
    "        t = resp['Table']\n",
    "        user_defined_pii = []\n",
    "        schema = []\n",
    "        for col in t[\"StorageDescriptor\"][\"Columns\"]:\n",
    "            is_pk = False\n",
    "            if 'pk' in (col.get('Comment','')).lower() or 'primary_key' in (col.get('Comment', '')).lower():\n",
    "                is_pk = True\n",
    "\n",
    "            is_fk = False\n",
    "            if 'fk' in (col.get('Comment', '')).lower() or 'foreign_key' in (col.get('Comment', '')).lower():\n",
    "                is_fk = True    \n",
    "\n",
    "            if 'pii_column' in (col.get('Comment', '')).lower():\n",
    "                user_defined_pii.append(col[\"Name\"])\n",
    "                \n",
    "\n",
    "            schema.append({\n",
    "                \"name\": col[\"Name\"],\n",
    "                \"type\": col[\"Type\"],\n",
    "                \"nullable\": False if is_pk else True,\n",
    "                \"partition_key\": False,\n",
    "                \"primary_key\" : is_pk,\n",
    "                \"foregin_key\" : is_fk, \n",
    "                \"comments\" : col.get(\"Comment\", \"\")\n",
    "            })\n",
    "  \n",
    "        for col in t.get(\"PartitionKeys\", []):\n",
    "            schema.append({\n",
    "                \"name\": col[\"Name\"],\n",
    "                \"type\": col[\"Type\"],\n",
    "                \"nullable\": True,\n",
    "                \"partition_key\": True,\n",
    "                \"primary_key\" : False,\n",
    "                \"foreign_key\" : False,\n",
    "                \"comments\" : col.get(\"Comment\", \"\")  \n",
    "            })\n",
    " \n",
    "        return ({\n",
    "            \"table_name\": t[\"Name\"],\n",
    "            \"database\": t[\"DatabaseName\"],\n",
    "            \"schema\": schema\n",
    "        },user_defined_pii)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"error in glue_metadata:\", str(e))\n",
    "        return ({}, [])\n",
    "\n",
    "def build_stats_sql(database, table, schema, pii_list):\n",
    "\n",
    "    pii_columns = set(c.lower() for c in pii_list)\n",
    "    selects = []\n",
    "\n",
    "    row_cnts = f\"\"\" \n",
    "                SELECT\n",
    "                'ROW_COUNT' as col_name,\n",
    "                CAST(COUNT(*) AS VARCHAR) AS min_val,\n",
    "                NULL as max_val,\n",
    "                NULL as null_pct,\n",
    "                NULL as distinct_count\n",
    "                FROM \"{database}\".\"{table}\"\n",
    "                \"\"\".strip()\n",
    "    selects.append(row_cnts)\n",
    "\n",
    "    for col in schema:\n",
    "        col_name = col[\"name\"]\n",
    "        col_lower = col_name.lower()\n",
    "\n",
    "        if col_lower in pii_columns:\n",
    "            continue\n",
    "\n",
    "        s = f\"\"\"\n",
    "            SELECT\n",
    "            '{col_name}' AS col_name,\n",
    "            CAST(MIN(\"{col_name}\") AS VARCHAR) AS min_val,\n",
    "            CAST(MAX(\"{col_name}\") AS VARCHAR) AS max_val,\n",
    "            AVG(CASE WHEN \"{col_name}\" IS NULL THEN 1.0 ELSE 0 END) AS null_pct,\n",
    "            APPROX_DISTINCT(\"{col_name}\") AS distinct_count\n",
    "            FROM \"{database}\".\"{table}\"\n",
    "            \"\"\".strip()\n",
    "\n",
    "        selects.append(s)\n",
    "\n",
    "    if not selects:\n",
    "        return None\n",
    "\n",
    "    sql = \"\\nUNION ALL\\n\".join(selects)\n",
    "    return sql\n",
    "\n",
    "\n",
    "def athena_setup(database,region,query,s3_bucket):\n",
    "    try:\n",
    "        athena_client = boto3.client('athena',region_name=region)\n",
    "\n",
    "        response = athena_client.start_query_execution(QueryString=query, QueryExecutionContext = {'Database' : database},\n",
    "        ResultConfiguration = {'OutputLocation':s3_bucket})\n",
    "\n",
    "        id = response['QueryExecutionId'] # generated by athena as a ticket number\n",
    "        \n",
    "        while True:\n",
    "            stats = athena_client.get_query_execution(QueryExecutionId=id)\n",
    "            status = stats['QueryExecution']['Status']['State']\n",
    "\n",
    "            if status =='SUCCEEDED':\n",
    "                break \n",
    "            elif status in ['FAILED', 'CANCELLED']:\n",
    "                reason = stats['QueryExecution']['Status'].get('StateChangeReason', 'Unknown')\n",
    "                raise Exception(f\"Query Failed: {reason}\")\n",
    "\n",
    "            time.sleep(5)\n",
    "\n",
    "        return athena_client.get_query_results(QueryExecutionId=id)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"error in run_athena_query:\", str(e))\n",
    "        return None \n",
    "\n",
    "# get the column value stats, runs on athena\n",
    "def get_athena_data(database, table, region, schema, pii_list, s3_bucket):\n",
    "    try:\n",
    "        sql = build_stats_sql(database, table, schema, pii_list)\n",
    "\n",
    "        if not sql:\n",
    "            print(\"no non-PII columns found for stats\")\n",
    "            return {}\n",
    "\n",
    "        query_out = athena_setup(database, region, sql, s3_bucket)\n",
    "\n",
    "        if not query_out:\n",
    "            return {}\n",
    "\n",
    "        rows = query_out[\"ResultSet\"][\"Rows\"]\n",
    "        col_stats = parse_stats_rows(rows)\n",
    "\n",
    "\n",
    "        return col_stats\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"error in get_athena_data:\", str(e))\n",
    "        return {}\n",
    "\n",
    "def parse_stats_rows(rows):\n",
    "    data_rows = rows[1:]\n",
    "\n",
    "    stats = {'ROW_COUNT' : 0}\n",
    "\n",
    "    for r in data_rows:\n",
    "        vals = [c.get(\"VarCharValue\") for c in r[\"Data\"]]\n",
    "\n",
    "\n",
    "        col_name = vals[0]\n",
    "        min_val = vals[1]\n",
    "        if col_name == 'ROW_COUNT':\n",
    "            stats['ROW_COUNT'] = min_val if min_val else 0\n",
    "            continue\n",
    "\n",
    "        max_val = vals[2]\n",
    "        null_pct = vals[3]\n",
    "        distinct = vals[4]\n",
    "\n",
    "        try:\n",
    "            null_pct = float(null_pct) if null_pct is not None else None\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            distinct = int(distinct) if distinct is not None else None\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        stats[col_name] = {\n",
    "            \"min\": min_val,\n",
    "            \"max\": max_val,\n",
    "            \"null_pct\": null_pct,\n",
    "            \"distinct_count\": distinct\n",
    "        }\n",
    "\n",
    "    return stats\n",
    "\n",
    "    \n",
    "\n",
    "# remove the pii information in the column stats from athena\n",
    "def filter_pii(col_stats, pii_list):\n",
    "    pii_columns = set(c.lower() for c in pii_list)\n",
    "\n",
    "    cleaned_stats = {}\n",
    "    for col_name, vals in col_stats.items():\n",
    "        if col_name.lower() not in pii_columns:\n",
    "            cleaned_stats[col_name] = vals\n",
    "\n",
    "    return cleaned_stats\n",
    "\n",
    "\n",
    "\n",
    "ddl_obj,table_level_pii = glue_metadata(database, table, region)\n",
    "if \n",
    "# schema = ddl_obj.get(\"schema\", [])\n",
    "\n",
    "# col_stats = get_athena_data(database, table, region, schema, USER_DEFINED_PII, s3_bucket)\n",
    "\n",
    "\n",
    "# col_stats = filter_pii(col_stats, USER_DEFINED_PII)\n",
    "\n",
    "# ddl_obj[\"column_stats\"] = col_stats\n",
    "\n",
    "# print(json.dumps(ddl_obj, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4304277d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emp_id']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_level_pii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "91604a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = f\"\"\"\n",
    "Pay Attention, Do not hallucinate, only work on what is there in the below and think deep for all the edge cases for the below requirements.\n",
    "Do NOT assume any properties (file format, update frequency, row counts, S3 location, etc.) that are not explicitly present in the JSON. If unknown, either omit or mark as \"unknown\".\n",
    "\n",
    "You are a Senior Data Engineer Copilot specializing in **AWS Glue and Athena**.\n",
    "\n",
    "You will receive a schema object extracted directly from the **AWS Glue Data Catalog**.\n",
    "Your goal is to generate a Data Quality Contract and Post-Load Tests suitable for an AWS Data Lake environment and also confulence style documentation for the table.\n",
    "\n",
    "The JSON object will look like this (shape, not exact values):\n",
    "\n",
    "{json.dumps(ddl_obj, indent=2)}\n",
    "\n",
    "Where:\n",
    "- table_name: name of the table\n",
    "- database: database / schema name\n",
    "- schema: list of columns, each like:\n",
    "  - name: column name\n",
    "  - type: data type (string, int, double, date, etc.)\n",
    "  - nullable: true / false\n",
    "  - comments: optional free-text description that may contain strict business rules\n",
    "- column_stats: ONLY for NON-PII columns, something like:\n",
    "  - min, max, null_pct, distinct_count, top_values\n",
    "- constraints: optional list of primary keys, unique keys, check constraints\n",
    "- job_summary: includes inputs, filters, and grain if available\n",
    "- rule_type MUST be exactly one of the allowed values. \n",
    "  Do NOT invent new rule_type names. If unsure, choose the closest one.\n",
    "\n",
    "IMPORTANT PRIVACY RULES:\n",
    "- PII columns (name, email, phone_number, salary, etc.) appear in \"schema\"\n",
    "  but their stats and values are NOT provided.\n",
    "- For PII columns:\n",
    "  - You MAY define structural rules: not_null, not_empty, length, regex.\n",
    "  - You MUST NOT include any concrete example values (no fake SSNs, emails, phones).\n",
    "  - Only describe patterns, like \"must be 9 digits\", \"must match email format\".\n",
    "- For non-PII columns:\n",
    "  - You MAY use column_stats to propose ranges and allowed_values.\n",
    "  - Still avoid writing specific sample values in descriptions; talk about rules.\n",
    "\n",
    "----------------------------------------\n",
    "INTERPRETING COLUMN COMMENTS (BUSINESS RULES)\n",
    "----------------------------------------\n",
    "\n",
    "For every column, you MUST read and interpret the \"comments\" field (if present).\n",
    "\n",
    "- If comments clearly indicate the column is required or cannot be null\n",
    "  (examples: \"cannot be null\", \"must not be null\", \"mandatory\", \"required\"),\n",
    "  then you MUST treat that column as **business not-null**, even if nullable = true.\n",
    "  In that case:\n",
    "    - Generate a not_null rule.\n",
    "    - spark_exp MUST enforce non-null, for example: \"joining_date IS NOT NULL\".\n",
    "\n",
    "- If comments describe value constraints, you MUST turn them into rules:\n",
    "  Examples:\n",
    "    - \"must be positive\" → min rule with spark_exp like \"col > 0\" (with NULL allowed if column is nullable).\n",
    "    - \"0–100 only\" → min + max, or an allowed_values rule.\n",
    "    - \"YYYY-MM-DD\" → format / regex rule on a string column, or date validity check.\n",
    "    - \"ISO country code (3-char)\" → length or regex rule.\n",
    "\n",
    "- If there is a conflict between nullable and comments:\n",
    "    - nullable = true but comments say \"cannot be null\" → comments WIN, you MUST enforce not_null.\n",
    "    - nullable = false but comments are silent → follow nullable = false as usual.\n",
    "\n",
    "- Comments can also add extra logic beyond nullability (e.g. business ranges, relations).\n",
    "  You MUST convert any clear business rule from comments into either:\n",
    "    - a data_quality rule (with spark_exp), and/or\n",
    "    - a post-load test.\n",
    "\n",
    "Do NOT ignore comments. If comments contain a clear rule, you must reflect it in the JSON.\n",
    "\n",
    "----------------------------------------\n",
    "THINKING / COVERAGE REQUIREMENTS\n",
    "----------------------------------------\n",
    "\n",
    "You must think column-by-column and constraint-by-constraint. \n",
    "Do not skip any column.\n",
    "For coverage:\n",
    "- Every column in \"schema\" (except purely technical partition columns) MUST appear in at least one rule in data_quality.rules.\n",
    "- Do NOT skip columns just because nullable = true. If a column is nullable, you can still enforce rules like \"if present, must not be empty\" or \"if present, must match pattern\".\n",
    "\n",
    "For every column in \"schema\":\n",
    "\n",
    "1. COMPLETENESS\n",
    "   IMPORTANT:\n",
    "    - If nullable = true AND comments do NOT say things like \"cannot be null\" / \"mandatory\" / \"required\":\n",
    "        * You MUST NOT create a not_null rule.\n",
    "        * Nullable columns must allow NULL in all spark_exp expressions.\n",
    "    - If nullable = true BUT comments explicitly say the column cannot be null (or equivalent):\n",
    "        * Treat it as business-required.\n",
    "        * You MUST create a not_null rule.\n",
    "        * spark_exp must enforce non-null, e.g. \"joining_date IS NOT NULL\".\n",
    "    - If nullable = false → always generate a not_null rule.\n",
    "\n",
    "   - For ALL string-like columns (string, varchar, char), regardless of nullable:\n",
    "       * Always generate a not_empty-style rule:\n",
    "         - If nullable = true: use logic \"value IS NULL OR trimmed length > 0\".\n",
    "         - If nullable = false OR business-required via comments: use logic \"value IS NOT NULL AND trimmed length > 0\".\n",
    "\n",
    "2. VALIDITY\n",
    "   Use the combination of:\n",
    "   - column name\n",
    "   - data type \n",
    "   - column_stats (only non-PII)\n",
    "   - constraints (CHECK, PK, UNIQUE)\n",
    "   - comments (business rules)\n",
    "   to infer validity rules such as:\n",
    "     * numeric columns >= 0 unless obviously not applicable\n",
    "     * string columns with stable lengths → infer min_length / max_length or regex\n",
    "     * year/date columns must not be in the future\n",
    "     * codes (country_code, dep_code) must be in allowed_values if low-cardinality\n",
    "     * any explicit range / format / business condition described in comments\n",
    "\n",
    "3. RANGE RULES (NON-PII ONLY)\n",
    "   Use column_stats[min_val, max_val, distinct_count, null_pct].\n",
    "   Create soft WARNING rules with a 20–25% buffer around min/max or p95 if present.\n",
    "\n",
    "4. ALLOWED VALUES (NON-PII ONLY)\n",
    "   If distinct_count is small (< 50) AND stable → generate allowed_values.\n",
    "\n",
    "5. PII COLUMNS\n",
    "   - PII columns appear in schema but have NO column_stats.\n",
    "   - For these columns you MUST generate:\n",
    "       * not_null (if nullable = false OR comments say it is required)\n",
    "       * not_empty for strings\n",
    "       * regex or fixed-length patterns inferred ONLY from schema + column name + comments\n",
    "   - NEVER include example email or SSN values. Only describe patterns.\n",
    "\n",
    "6. CROSS-COLUMN LOGIC (IF OBVIOUS)\n",
    "   If year/date columns exist → ensure year <= current year.\n",
    "   If ID + email exist → email should not be null if ID exists.\n",
    "   If joining_date and resign_date exist → resign_date >= joining_date.\n",
    "   If comments describe cross-column relationships, you MUST convert those into rules or tests.\n",
    "\n",
    "7. TABLE-LEVEL RULES\n",
    "   - If constraints include primary key → include uniqueness rule.\n",
    "   - Add table-level rule: row_count > 0.\n",
    "\n",
    "8. Data Quality (pre-load PySpark)\n",
    "   For each rule, you MUST output \"spark_exp\" using **Spark SQL syntax only**, not PySpark API.\n",
    "    spark_exp MUST be a SQL expression that can be passed directly into:\n",
    "\n",
    "    df.filter(expr(spark_exp))\n",
    "\n",
    "    Examples of valid spark_exp:\n",
    "      \"salary >= 0\"\n",
    "      \"salary IS NULL OR salary >= 0\"\n",
    "      \"name IS NULL OR length(trim(name)) > 0\"\n",
    "      \"joining_date <= current_date()\"\n",
    "      \"joining_date IS NOT NULL\"   -- when comments say 'cannot be null'\n",
    "\n",
    "    Examples of INVALID spark_exp (do NOT generate these):\n",
    "      col('salary') >= 0\n",
    "      F.col(\"name\").isNull()\n",
    "      dataframe.count() > 0\n",
    "      salary.notNull()\n",
    "\n",
    "9. TEST COVERAGE (Post-load SQL)\n",
    "   You must generate SQL tests for:\n",
    "       * uniqueness of PK/grain\n",
    "       * null checks on required columns (nullable=false OR required via comments)\n",
    "       * each CHECK constraint\n",
    "       * future-date violations\n",
    "       * allowed_values validation (for low-cardinality columns)\n",
    "       * numeric range violations\n",
    "\n",
    "After generating rules and tests, REVIEW:\n",
    "- Did you include ALL columns?\n",
    "- Did you cover ALL non-nullable columns and all columns required by comments?\n",
    "- Did you enforce ALL constraints + business rules from comments?\n",
    "- Did you create BOTH rules AND tests?\n",
    "\n",
    "\n",
    "----------------------------------------\n",
    "OUTPUT FORMAT (MUST BE VALID JSON)\n",
    "----------------------------------------\n",
    "Important: In the final JSON, the set of column names used in data_quality.rules (excluding \"__TABLE__\") MUST match the set of column names in \"schema\" (case-insensitive). Do not omit any columns.\n",
    "           Coverage requirement does NOT override the schema.\n",
    "           If a column is nullable and comments do not make it required,\n",
    "           you may generate \"if present, must...\" rules\n",
    "           (e.g., not_empty, min/max with NULL allowed),\n",
    "           but you MUST NOT force mandatory constraints such as not_null.\n",
    "\n",
    "Return ONLY valid JSON in this exact structure (no extra comments):\n",
    "\n",
    "{{\n",
    "  \"data_quality\": {{\n",
    "    \"rules\": [\n",
    "      {{\n",
    "        \"column\": \"col_name_or__TABLE__for_table_level\",\n",
    "        \"rule_type\": \"not_null | not_empty | min | max | allowed_values | regex | pk | fk | check_constraint | custom_sql\",\n",
    "        \"condition\": \"value / list / SQL expression / description string\",\n",
    "        \"severity\": \"ERROR | WARNING\",\n",
    "        \"action\": \"FAIL_JOB | DROP_ROW | WARN\",\n",
    "        \"description\": \"Short reasoning for the rule (no concrete example values).\",\n",
    "        \"spark_exp\": \"A Spark SQL boolean expression that returns TRUE for valid rows and can be passed directly to pyspark.sql.functions.expr(). It MUST NOT reference any DataFrame variable and MUST NOT call actions like count(), groupBy(), collect(), etc.\"\n",
    "      }}\n",
    "    ]\n",
    "  }},\n",
    "  \"tests\": [\n",
    "    {{\n",
    "      \"name\": \"test_name\",\n",
    "      \"sql\": \"SELECT ...\",\n",
    "      \"description\": \"What this test validates.\"\n",
    "    }}\n",
    "  ],\n",
    "  \"docs_markdown\": \"# Table Documentation\\\\n...\"\n",
    "}}\n",
    "\n",
    "Do NOT include anything outside this JSON object.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6384c1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"data_quality\": {\n",
      "    \"rules\": [\n",
      "      {\n",
      "        \"column\": \"__TABLE__\",\n",
      "        \"rule_type\": \"custom_sql\",\n",
      "        \"condition\": \"row_count > 0\",\n",
      "        \"severity\": \"ERROR\",\n",
      "        \"action\": \"FAIL_JOB\",\n",
      "        \"description\": \"The table must not be empty after loading.\",\n",
      "        \"spark_exp\": \"true\"\n",
      "      },\n",
      "      {\n",
      "        \"column\": \"emp_id\",\n",
      "        \"rule_type\": \"not_null\",\n",
      "        \"condition\": \"Must not be null\",\n",
      "        \"severity\": \"ERROR\",\n",
      "        \"action\": \"FAIL_JOB\",\n",
      "        \"description\": \"Employee ID is the primary key and cannot be null.\",\n",
      "        \"spark_exp\": \"emp_id IS NOT NULL\"\n",
      "      },\n",
      "      {\n",
      "        \"column\": \"emp_id\",\n",
      "        \"rule_type\": \"pk\",\n",
      "        \"condition\": \"Must be unique\",\n",
      "        \"severity\": \"ERROR\",\n",
      "        \"action\": \"FAIL_JOB\",\n",
      "        \"description\": \"Employee ID must be unique across all records.\",\n",
      "        \"spark_exp\": \"true\"\n",
      "      },\n",
      "      {\n",
      "        \"column\": \"emp_id\",\n",
      "        \"rule_type\": \"min\",\n",
      "        \"condition\": \"1\",\n",
      "        \"severity\": \"ERROR\",\n",
      "        \"action\": \"DROP_ROW\",\n",
      "        \"description\": \"Employee ID must be a positive integer.\",\n",
      "        \"spark_exp\": \"emp_id > 0\"\n",
      "      },\n",
      "      {\n",
      "        \"column\": \"name\",\n",
      "        \"rule_type\": \"not_empty\",\n",
      "        \"condition\": \"Must not be an empty string\",\n",
      "        \"severity\": \"WARNING\",\n",
      "        \"action\": \"WARN\",\n",
      "        \"description\": \"If present, the name field should not be empty or contain only whitespace. This is a PII column.\",\n",
      "        \"spark_exp\": \"name IS NULL OR length(trim(name)) > 0\"\n",
      "      },\n",
      "      {\n",
      "        \"column\": \"salary\",\n",
      "        \"rule_type\": \"min\",\n",
      "        \"condition\": \"0\",\n",
      "        \"severity\": \"WARNING\",\n",
      "        \"action\": \"WARN\",\n",
      "        \"description\": \"If present, salary must be a non-negative value. This is a PII column.\",\n",
      "        \"spark_exp\": \"salary IS NULL OR salary >= 0\"\n",
      "      },\n",
      "      {\n",
      "        \"column\": \"department\",\n",
      "        \"rule_type\": \"not_empty\",\n",
      "        \"condition\": \"Must not be an empty string\",\n",
      "        \"severity\": \"WARNING\",\n",
      "        \"action\": \"WARN\",\n",
      "        \"description\": \"If present, the department field should not be empty or contain only whitespace.\",\n",
      "        \"spark_exp\": \"department IS NULL OR length(trim(department)) > 0\"\n",
      "      },\n",
      "      {\n",
      "        \"column\": \"joining_date\",\n",
      "        \"rule_type\": \"not_null\",\n",
      "        \"condition\": \"Must not be null\",\n",
      "        \"severity\": \"ERROR\",\n",
      "        \"action\": \"DROP_ROW\",\n",
      "        \"description\": \"Joining date is a mandatory field as per business rule in comments, overriding schema nullability.\",\n",
      "        \"spark_exp\": \"joining_date IS NOT NULL\"\n",
      "      },\n",
      "      {\n",
      "        \"column\": \"joining_date\",\n",
      "        \"rule_type\": \"max\",\n",
      "        \"condition\": \"current_date()\",\n",
      "        \"severity\": \"ERROR\",\n",
      "        \"action\": \"DROP_ROW\",\n",
      "        \"description\": \"Joining date cannot be in the future.\",\n",
      "        \"spark_exp\": \"joining_date <= current_date()\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"tests\": [\n",
      "    {\n",
      "      \"name\": \"test_pk_uniqueness_emp_id\",\n",
      "      \"sql\": \"SELECT emp_id, COUNT(*) FROM copilot_demo.employees_test GROUP BY emp_id HAVING COUNT(*) > 1\",\n",
      "      \"description\": \"Verifies that the primary key 'emp_id' is unique across the entire table. This query should return 0 rows.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"test_not_null_emp_id\",\n",
      "      \"sql\": \"SELECT COUNT(*) FROM copilot_demo.employees_test WHERE emp_id IS NULL\",\n",
      "      \"description\": \"Verifies that the 'emp_id' column contains no NULL values. This query should return 0.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"test_business_not_null_joining_date\",\n",
      "      \"sql\": \"SELECT COUNT(*) FROM copilot_demo.employees_test WHERE joining_date IS NULL\",\n",
      "      \"description\": \"Verifies that the 'joining_date' column contains no NULL values, as per business requirements. This query should return 0.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"test_future_date_joining_date\",\n",
      "      \"sql\": \"SELECT COUNT(*) FROM copilot_demo.employees_test WHERE joining_date > current_date\",\n",
      "      \"description\": \"Verifies that no records have a joining date in the future. This query should return 0.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"test_cardinality_department\",\n",
      "      \"sql\": \"SELECT COUNT(1) FROM (SELECT COUNT(DISTINCT department) AS distinct_departments FROM copilot_demo.employees_test) WHERE distinct_departments > 6\",\n",
      "      \"description\": \"Verifies that the number of distinct departments does not exceed the known cardinality of 6. A non-zero result may indicate new, un-validated department values.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"test_positive_emp_id\",\n",
      "      \"sql\": \"SELECT COUNT(*) FROM copilot_demo.employees_test WHERE emp_id <= 0\",\n",
      "      \"description\": \"Verifies that all 'emp_id' values are positive. This query should return 0.\"\n",
      "    }\n",
      "  ],\n",
      "  \"docs_markdown\": \"# Table: employees_test\\n\\n**Database:** `copilot_demo`\\n\\n## 1. Overview\\n\\nThis table contains employee information, including their ID, name, salary, department, and start date. It serves as a core dataset for HR analytics and reporting.\\n\\n| Property | Value |\\n| :--- | :--- |\\n| **Data Steward** | unknown |\\n| **Source System** | unknown |\\n| **Update Frequency** | unknown |\\n| **S3 Location** | unknown |\\n| **File Format** | unknown |\\n\\n## 2. Schema\\n\\n| Column Name | Data Type | Nullable | PII | Description |\\n| :--- | :--- | :--- | :--- | :--- |\\n| `emp_id` | `int` | No | No | **Primary Key.** Unique identifier for each employee. |\\n| `name` | `string` | Yes | Yes | The full name of the employee. |\\n| `salary` | `double` | Yes | Yes | The employee's current salary. |\\n| `department` | `string` | Yes | No | The department the employee belongs to. Expected to be one of 6 distinct values. |\\n| `joining_date` | `date` | **Yes (No)** | No | The date the employee joined the company. **Business Rule:** This column cannot be null, despite the technical schema allowing it. |\\n\\n## 3. Key Business Rules & Data Quality\\n\\nThis table has several critical data quality rules enforced during the data loading process:\\n\\n*   **Employee ID (`emp_id`)**: Must be a unique, non-null, positive integer.\\n*   **Joining Date (`joining_date`)**: Is a mandatory field and cannot be null. It also cannot be a future date. Records violating this rule will be dropped.\\n*   **PII Fields (`name`, `salary`)**: While nullable, these fields are monitored. `name` should not be an empty string if provided, and `salary` must be non-negative.\\n*   **Department (`department`)**: This is a low-cardinality field. The number of distinct departments is monitored to detect unexpected changes.\\n\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "client = genai.Client()\n",
    "response = client.models.generate_content(model='gemini-2.5-pro', contents=payload)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602e6ddc",
   "metadata": {},
   "source": [
    "# Push to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5624404",
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_json_path = \"s3://de-copilot-s3/contracts/\"\n",
    "s3_client = boto3.client('s3',region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e7abda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_json_str = re.sub(r\"```json\\n|\\n```\", \"\", response.text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5b1ab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_json = json.loads(clean_json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32141ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'MAY7KSGK88MB18RK',\n",
       "  'HostId': 'oZ+X5mZ309qKCKN8nAE6DDziYBla5fBjNq0L/boUIj0EyUuNIqx862uw2uhEkh55VtH6PudYUH4fhR83r/D9t/A1S9BoGXZv',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'oZ+X5mZ309qKCKN8nAE6DDziYBla5fBjNq0L/boUIj0EyUuNIqx862uw2uhEkh55VtH6PudYUH4fhR83r/D9t/A1S9BoGXZv',\n",
       "   'x-amz-request-id': 'MAY7KSGK88MB18RK',\n",
       "   'date': 'Sun, 23 Nov 2025 18:58:33 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"b185c99ce1de48a934ecb2165038253f\"',\n",
       "   'x-amz-checksum-crc32': '8+l7aA==',\n",
       "   'x-amz-checksum-type': 'FULL_OBJECT',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"b185c99ce1de48a934ecb2165038253f\"',\n",
       " 'ChecksumCRC32': '8+l7aA==',\n",
       " 'ChecksumType': 'FULL_OBJECT',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_client.put_object(Bucket='de-copilot-s3',Key=f'contracts/{table}.json',Body=json.dumps(cleaned_json,indent=2),ContentType='application/json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9176f2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'table_name': 'employees_test',\n",
       " 'database': 'copilot_demo',\n",
       " 'schema': [{'name': 'emp_id',\n",
       "   'type': 'int',\n",
       "   'nullable': False,\n",
       "   'partition_key': False,\n",
       "   'primary_key': True,\n",
       "   'foregin_key': False,\n",
       "   'comments': 'primary_key'},\n",
       "  {'name': 'name',\n",
       "   'type': 'string',\n",
       "   'nullable': True,\n",
       "   'partition_key': False,\n",
       "   'primary_key': False,\n",
       "   'foregin_key': False,\n",
       "   'comments': ''},\n",
       "  {'name': 'salary',\n",
       "   'type': 'double',\n",
       "   'nullable': True,\n",
       "   'partition_key': False,\n",
       "   'primary_key': False,\n",
       "   'foregin_key': False,\n",
       "   'comments': ''},\n",
       "  {'name': 'department',\n",
       "   'type': 'string',\n",
       "   'nullable': True,\n",
       "   'partition_key': False,\n",
       "   'primary_key': False,\n",
       "   'foregin_key': False,\n",
       "   'comments': ''},\n",
       "  {'name': 'joining_date',\n",
       "   'type': 'date',\n",
       "   'nullable': True,\n",
       "   'partition_key': False,\n",
       "   'primary_key': False,\n",
       "   'foregin_key': False,\n",
       "   'comments': 'cannot be null'}],\n",
       " 'column_stats': {'ROW_COUNT': '261',\n",
       "  'joining_date': {'min': '2011-01-17',\n",
       "   'max': '2023-11-30',\n",
       "   'null_pct': 0.0038314176245210726,\n",
       "   'distinct_count': 250},\n",
       "  'department': {'min': 'Finance',\n",
       "   'max': 'Support',\n",
       "   'null_pct': 0.0,\n",
       "   'distinct_count': 6},\n",
       "  'emp_id': {'min': '1',\n",
       "   'max': '287',\n",
       "   'null_pct': 0.0,\n",
       "   'distinct_count': 258}}}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddl_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42563993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
