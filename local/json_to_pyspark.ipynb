{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4558912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/21 17:58:39 WARN Utils: Your hostname, LAPTOP-E66VD905, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/11/21 17:58:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/21 17:58:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import pyspark\n",
    "import copy\n",
    "from pyspark.sql import functions as f \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "spark = SparkSession.builder.appName('de-copilot').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffb268b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3',region_name = 'us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cbfc257",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts = s3_client.get_object(Bucket='de-copilot-s3', Key='contracts/employees_test.json')\n",
    "contracts = contracts['Body'].read().decode('utf-8')\n",
    "contracts = json.loads(contracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c042c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rules = contracts.get('data_quality',{}).get('rules',[])\n",
    "final_rules = []\n",
    "\n",
    "for cur_rule in all_rules:\n",
    "    cur_col = cur_rule.get('column','')\n",
    "    cur_rtype = cur_rule.get('rule_type','')\n",
    "    cur_spark_exp = cur_rule.get('spark_exp')\n",
    "    if cur_spark_exp:\n",
    "        if cur_col.upper() =='__TABLE__' or (cur_rtype in ('pk','fk')) or (cur_spark_exp.lower() == 'true'):\n",
    "            continue \n",
    "        final_rules.append(cur_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87daac8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'column': 'emp_id',\n",
       "  'rule_type': 'not_null',\n",
       "  'condition': 'must not be null',\n",
       "  'severity': 'ERROR',\n",
       "  'action': 'FAIL_JOB',\n",
       "  'description': 'Employee ID is a required field and serves as the primary key.',\n",
       "  'spark_exp': 'emp_id IS NOT NULL'},\n",
       " {'column': 'emp_id',\n",
       "  'rule_type': 'min',\n",
       "  'condition': '0',\n",
       "  'severity': 'ERROR',\n",
       "  'action': 'DROP_ROW',\n",
       "  'description': 'Employee ID must be a positive integer.',\n",
       "  'spark_exp': 'emp_id > 0'},\n",
       " {'column': 'name',\n",
       "  'rule_type': 'not_empty',\n",
       "  'condition': 'if present, must not be empty or whitespace',\n",
       "  'severity': 'WARNING',\n",
       "  'action': 'WARN',\n",
       "  'description': 'Employee name, if provided, must contain non-whitespace characters.',\n",
       "  'spark_exp': 'name IS NULL OR length(trim(name)) > 0'},\n",
       " {'column': 'salary',\n",
       "  'rule_type': 'min',\n",
       "  'condition': '0',\n",
       "  'severity': 'WARNING',\n",
       "  'action': 'WARN',\n",
       "  'description': 'Salary, if provided, must be a non-negative value.',\n",
       "  'spark_exp': 'salary IS NULL OR salary >= 0'},\n",
       " {'column': 'department',\n",
       "  'rule_type': 'not_empty',\n",
       "  'condition': 'if present, must not be empty or whitespace',\n",
       "  'severity': 'WARNING',\n",
       "  'action': 'WARN',\n",
       "  'description': 'Department name, if provided, must contain non-whitespace characters.',\n",
       "  'spark_exp': 'department IS NULL OR length(trim(department)) > 0'},\n",
       " {'column': 'joining_date',\n",
       "  'rule_type': 'custom_sql',\n",
       "  'condition': '<= current_date()',\n",
       "  'severity': 'ERROR',\n",
       "  'action': 'DROP_ROW',\n",
       "  'description': 'The joining date cannot be in the future.',\n",
       "  'spark_exp': 'joining_date IS NULL OR joining_date <= current_date()'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "90423f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "severity_checks = []\n",
    "for rule in final_rules:\n",
    "    spark_exp = rule['spark_exp']\n",
    "    error_msg = rule['description']\n",
    "    severity = rule['severity']\n",
    "\n",
    "    cond = f.when(expr(f'NOT ({spark_exp})'), f.lit(error_msg)).otherwise(f.lit(None))\n",
    "    errors.append(cond)\n",
    "\n",
    "    sev_cond = f.when(expr(f'NOT ({spark_exp})'), f.lit(severity)).otherwise(f.lit(None))\n",
    "    severity_checks.append(sev_cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a51eb955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column<'CASE WHEN NOT (emp_id IS NOT NULL) THEN 'Employee ID is a required field and serves as the primary key.' ELSE NULL END'>,\n",
       " Column<'CASE WHEN NOT (emp_id > 0) THEN 'Employee ID must be a positive integer.' ELSE NULL END'>,\n",
       " Column<'CASE WHEN NOT (name IS NULL OR length(trim(name)) > 0) THEN 'Employee name, if provided, must contain non-whitespace characters.' ELSE NULL END'>,\n",
       " Column<'CASE WHEN NOT (salary IS NULL OR salary >= 0) THEN 'Salary, if provided, must be a non-negative value.' ELSE NULL END'>,\n",
       " Column<'CASE WHEN NOT (department IS NULL OR length(trim(department)) > 0) THEN 'Department name, if provided, must contain non-whitespace characters.' ELSE NULL END'>,\n",
       " Column<'CASE WHEN NOT (joining_date IS NULL OR joining_date <= current_date()) THEN 'The joining date cannot be in the future.' ELSE NULL END'>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2db0ceea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column<'CASE WHEN NOT (emp_id IS NOT NULL) THEN 'ERROR' ELSE NULL END'>,\n",
       " Column<'CASE WHEN NOT (emp_id > 0) THEN 'ERROR' ELSE NULL END'>,\n",
       " Column<'CASE WHEN NOT (name IS NULL OR length(trim(name)) > 0) THEN 'WARNING' ELSE NULL END'>,\n",
       " Column<'CASE WHEN NOT (salary IS NULL OR salary >= 0) THEN 'WARNING' ELSE NULL END'>,\n",
       " Column<'CASE WHEN NOT (department IS NULL OR length(trim(department)) > 0) THEN 'WARNING' ELSE NULL END'>,\n",
       " Column<'CASE WHEN NOT (joining_date IS NULL OR joining_date <= current_date()) THEN 'ERROR' ELSE NULL END'>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "severity_checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b168109",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3b0b8515",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (101, \"Alice\", 50000.0, \"IT\", \"2023-01-01\"),\n",
    "    (102, \"Bob\",  -100.0,   \"HR\", \"2023-01-01\"), \n",
    "    (103, None,    60000.0, \"Sales\", \"2023-01-01\")\n",
    "]\n",
    "columns = [\"emp_id\", \"name\", \"salary\", \"department\", \"joining_date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "546f0b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data = data,schema = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5aebcd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('all_errors', f.array(*(errors))).withColumn('severity_check',f.array(*(severity_checks)))\\\n",
    "    .withColumn('Reason',expr(\"filter(all_errors, x -> x is Not Null)\"))\\\n",
    "    .withColumn('dq_severity',expr(\"filter(severity_check, x -> x is Not Null)\")).drop('all_errors','severity_check')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "be45125e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+----------+------------+----------------------------------------------------+-----------+\n",
      "|emp_id|name |salary |department|joining_date|Reason                                              |dq_severity|\n",
      "+------+-----+-------+----------+------------+----------------------------------------------------+-----------+\n",
      "|101   |Alice|50000.0|IT        |2023-01-01  |[]                                                  |[]         |\n",
      "|102   |Bob  |-100.0 |HR        |2023-01-01  |[Salary, if provided, must be a non-negative value.]|[WARNING]  |\n",
      "|103   |NULL |60000.0|Sales     |2023-01-01  |[]                                                  |[]         |\n",
      "+------+-----+-------+----------+------------+----------------------------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dd59263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = df.filter(f.size('Reason')==0).drop('Reason')\n",
    "df_invalid = df.filter(f.size('Reason')>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c6e1aee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+----------+------------+----------------------------------------------------+-----------+\n",
      "|emp_id|name|salary|department|joining_date|Reason                                              |dq_severity|\n",
      "+------+----+------+----------+------------+----------------------------------------------------+-----------+\n",
      "|102   |Bob |-100.0|HR        |2023-01-01  |[Salary, if provided, must be a non-negative value.]|[WARNING]  |\n",
      "+------+----+------+----------+------------+----------------------------------------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_invalid.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6096f3c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
