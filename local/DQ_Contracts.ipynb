{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55d8b982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import boto3\n",
    "import time\n",
    "import json, copy\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "300ffd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = 'copilot_demo'\n",
    "table = 'employees_test'\n",
    "region = 'us-east-1'\n",
    "default_pii = [\"name\", \"email\", \"phone_number\", \"salary\"]\n",
    "athena_output = \"s3://de-copilot-s3/athena-results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b88cecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_client = boto3.client('glue', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1fe4858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"table_name\": \"employees_test\",\n",
      "  \"database\": \"copilot_demo\",\n",
      "  \"schema\": [\n",
      "    {\n",
      "      \"name\": \"emp_id\",\n",
      "      \"type\": \"int\",\n",
      "      \"nullable\": true,\n",
      "      \"partition_key\": false,\n",
      "      \"primary_key\": false,\n",
      "      \"foregin_key\": false,\n",
      "      \"comments\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"name\",\n",
      "      \"type\": \"string\",\n",
      "      \"nullable\": true,\n",
      "      \"partition_key\": false,\n",
      "      \"primary_key\": false,\n",
      "      \"foregin_key\": false,\n",
      "      \"comments\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"salary\",\n",
      "      \"type\": \"double\",\n",
      "      \"nullable\": true,\n",
      "      \"partition_key\": false,\n",
      "      \"primary_key\": false,\n",
      "      \"foregin_key\": false,\n",
      "      \"comments\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"department\",\n",
      "      \"type\": \"string\",\n",
      "      \"nullable\": true,\n",
      "      \"partition_key\": false,\n",
      "      \"primary_key\": false,\n",
      "      \"foregin_key\": false,\n",
      "      \"comments\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"joining_date\",\n",
      "      \"type\": \"date\",\n",
      "      \"nullable\": true,\n",
      "      \"partition_key\": false,\n",
      "      \"primary_key\": false,\n",
      "      \"foregin_key\": false,\n",
      "      \"comments\": \"\"\n",
      "    }\n",
      "  ],\n",
      "  \"row_count\": \"260\",\n",
      "  \"column_stats\": {\n",
      "    \"joining_date\": {\n",
      "      \"min\": \"2011-01-17\",\n",
      "      \"max\": \"2023-11-30\",\n",
      "      \"null_pct\": 0.0,\n",
      "      \"distinct_count\": 250\n",
      "    },\n",
      "    \"department\": {\n",
      "      \"min\": \"Finance\",\n",
      "      \"max\": \"Support\",\n",
      "      \"null_pct\": 0.0,\n",
      "      \"distinct_count\": 6\n",
      "    },\n",
      "    \"emp_id\": {\n",
      "      \"min\": \"1\",\n",
      "      \"max\": \"260\",\n",
      "      \"null_pct\": 0.0,\n",
      "      \"distinct_count\": 256\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# metadeta stats of the tables in the glue\n",
    "def glue_metadata(database, table, region):\n",
    "    try:\n",
    "        resp = glue_client.get_table(DatabaseName=database, Name=table)\n",
    "        t = resp['Table']\n",
    "        user_defined_pii = []\n",
    "        schema = []\n",
    "        for col in t[\"StorageDescriptor\"][\"Columns\"]:\n",
    "            is_pk = False\n",
    "            if 'pk' in (col.get('Comment','')).lower() or 'primary_key' in (col.get('Comment', '')).lower():\n",
    "                is_pk = True\n",
    "\n",
    "            is_fk = False\n",
    "            if 'fk' in (col.get('Comment', '')).lower() or 'foreign_key' in (col.get('Comment', '')).lower():\n",
    "                is_fk = True    \n",
    "\n",
    "            if 'pii_column' in (col.get('Comment', '')).lower():\n",
    "                user_defined_pii.append(col[\"Name\"])\n",
    "                \n",
    "\n",
    "            schema.append({\n",
    "                \"name\": col[\"Name\"],\n",
    "                \"type\": col[\"Type\"],\n",
    "                \"nullable\": False if is_pk else True,\n",
    "                \"partition_key\": False,\n",
    "                \"primary_key\" : is_pk,\n",
    "                \"foregin_key\" : is_fk, \n",
    "                \"comments\" : col.get(\"Comment\", \"\")\n",
    "            })\n",
    "  \n",
    "        for col in t.get(\"PartitionKeys\", []):\n",
    "            schema.append({\n",
    "                \"name\": col[\"Name\"],\n",
    "                \"type\": col[\"Type\"],\n",
    "                \"nullable\": True,\n",
    "                \"partition_key\": True,\n",
    "                \"primary_key\" : False,\n",
    "                \"foreign_key\" : False,\n",
    "                \"comments\" : col.get(\"Comment\", \"\")  \n",
    "            })\n",
    " \n",
    "        return ({\n",
    "            \"table_name\": t[\"Name\"],\n",
    "            \"database\": t[\"DatabaseName\"],\n",
    "            \"schema\": schema\n",
    "        },user_defined_pii)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"error in glue_metadata:\", str(e))\n",
    "        return ({}, [])\n",
    "\n",
    "def build_stats_sql(database, table, schema, pii_list):\n",
    "\n",
    "    pii_columns = set(c.lower() for c in pii_list)\n",
    "    selects = []\n",
    "\n",
    "    row_cnts = f\"\"\" \n",
    "                SELECT\n",
    "                'ROW_COUNT' as col_name,\n",
    "                CAST(COUNT(*) AS VARCHAR) AS min_val,\n",
    "                NULL as max_val,\n",
    "                NULL as null_pct,\n",
    "                NULL as distinct_count\n",
    "                FROM \"{database}\".\"{table}\"\n",
    "                \"\"\".strip()\n",
    "    selects.append(row_cnts)\n",
    "\n",
    "    for col in schema:\n",
    "        col_name = col[\"name\"]\n",
    "        col_lower = col_name.lower()\n",
    "\n",
    "        if col_lower in pii_columns:\n",
    "            continue\n",
    "\n",
    "        s = f\"\"\"\n",
    "            SELECT\n",
    "            '{col_name}' AS col_name,\n",
    "            CAST(MIN(\"{col_name}\") AS VARCHAR) AS min_val,\n",
    "            CAST(MAX(\"{col_name}\") AS VARCHAR) AS max_val,\n",
    "            AVG(CASE WHEN \"{col_name}\" IS NULL THEN 1.0 ELSE 0 END) AS null_pct,\n",
    "            APPROX_DISTINCT(\"{col_name}\") AS distinct_count\n",
    "            FROM \"{database}\".\"{table}\"\n",
    "            \"\"\".strip()\n",
    "\n",
    "        selects.append(s)\n",
    "\n",
    "    if not selects:\n",
    "        return None\n",
    "\n",
    "    sql = \"\\nUNION ALL\\n\".join(selects)\n",
    "    return sql\n",
    "\n",
    "\n",
    "def athena_setup(database,region,query,s3_bucket):\n",
    "    try:\n",
    "        athena_client = boto3.client('athena',region_name=region)\n",
    "\n",
    "        response = athena_client.start_query_execution(QueryString=query, QueryExecutionContext = {'Database' : database},\n",
    "        ResultConfiguration = {'OutputLocation':s3_bucket})\n",
    "\n",
    "        id = response['QueryExecutionId'] # generated by athena as a ticket number\n",
    "        \n",
    "        while True:\n",
    "            stats = athena_client.get_query_execution(QueryExecutionId=id)\n",
    "            status = stats['QueryExecution']['Status']['State']\n",
    "\n",
    "            if status =='SUCCEEDED':\n",
    "                break \n",
    "            elif status in ['FAILED', 'CANCELLED']:\n",
    "                reason = stats['QueryExecution']['Status'].get('StateChangeReason', 'Unknown')\n",
    "                raise Exception(f\"Query Failed: {reason}\")\n",
    "\n",
    "            time.sleep(5)\n",
    "\n",
    "        return athena_client.get_query_results(QueryExecutionId=id)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"error in run_athena_query:\", str(e))\n",
    "        return None \n",
    "\n",
    "# get the column value stats, runs on athena\n",
    "def get_athena_data(database, table, region, schema, pii_list, s3_bucket):\n",
    "    try:\n",
    "        sql = build_stats_sql(database, table, schema, pii_list)\n",
    "\n",
    "        if not sql:\n",
    "            print(\"no non-PII columns found for stats\")\n",
    "            return {}\n",
    "\n",
    "        query_out = athena_setup(database, region, sql, s3_bucket)\n",
    "\n",
    "        if not query_out:\n",
    "            return {}\n",
    "\n",
    "        rows = query_out[\"ResultSet\"][\"Rows\"]\n",
    "        col_stats = parse_stats_rows(rows)\n",
    "\n",
    "\n",
    "        return col_stats\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"error in get_athena_data:\", str(e))\n",
    "        return {}\n",
    "\n",
    "def parse_stats_rows(rows):\n",
    "    data_rows = rows[1:]\n",
    "\n",
    "    stats = {'ROW_COUNT' : 0}\n",
    "\n",
    "    for r in data_rows:\n",
    "        vals = [c.get(\"VarCharValue\") for c in r[\"Data\"]]\n",
    "\n",
    "\n",
    "        col_name = vals[0]\n",
    "        min_val = vals[1]\n",
    "        if col_name == 'ROW_COUNT':\n",
    "            stats['ROW_COUNT'] = min_val if min_val else 0\n",
    "            continue\n",
    "\n",
    "        max_val = vals[2]\n",
    "        null_pct = vals[3]\n",
    "        distinct = vals[4]\n",
    "\n",
    "        try:\n",
    "            null_pct = float(null_pct) if null_pct is not None else None\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            distinct = int(distinct) if distinct is not None else None\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        stats[col_name] = {\n",
    "            \"min\": min_val,\n",
    "            \"max\": max_val,\n",
    "            \"null_pct\": null_pct,\n",
    "            \"distinct_count\": distinct\n",
    "        }\n",
    "\n",
    "    return stats\n",
    "\n",
    "    \n",
    "\n",
    "# remove the pii information in the column stats from athena\n",
    "def filter_pii(col_stats, pii_list):\n",
    "    pii_columns = set(c.lower() for c in pii_list)\n",
    "\n",
    "    cleaned_stats = {}\n",
    "    for col_name, vals in col_stats.items():\n",
    "        if col_name.lower() not in pii_columns:\n",
    "            cleaned_stats[col_name] = vals\n",
    "\n",
    "    return cleaned_stats\n",
    "\n",
    "ddl_obj, user_defined_pii = glue_metadata(database, table, region)\n",
    "if not ddl_obj:\n",
    "    print('No DDL')\n",
    "\n",
    "if not user_defined_pii:\n",
    "    user_defined_pii = default_pii\n",
    "\n",
    "schema = ddl_obj.get(\"schema\", [])\n",
    "col_stats = get_athena_data(database, table, region, schema, user_defined_pii, athena_output)\n",
    "\n",
    "clean_stats = filter_pii(col_stats, user_defined_pii)\n",
    "\n",
    "row_count = clean_stats.get(\"ROW_COUNT\", 0)\n",
    "column_stats = {k: v for k, v in clean_stats.items() if k != \"ROW_COUNT\"}\n",
    "\n",
    "ddl_obj[\"row_count\"] = row_count\n",
    "ddl_obj[\"column_stats\"] = column_stats\n",
    "\n",
    "print(json.dumps(ddl_obj, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91604a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = \"\"\"\n",
    "Pay Attention, Do not hallucinate, only work on what is there in the below and think deep for all the edge cases for the below requirements.\n",
    "Do NOT assume any properties (file format, update frequency, row counts, S3 location, etc.) that are not explicitly present in the JSON. If unknown, either omit or mark as \"unknown\".\n",
    "\n",
    "You are a Senior Data Engineer Copilot specializing in **AWS Glue and Athena**.\n",
    "\n",
    "You will receive a schema object extracted directly from the **AWS Glue Data Catalog**.\n",
    "Your goal is to generate:\n",
    "- A Data Quality Contract (pre-load rules with Spark SQL expressions),\n",
    "- Post-Load Tests (Athena SQL),\n",
    "- Confluence-style documentation for the table,\n",
    "- And a separate \"suggestions\" section with extra OPTIONAL rules for human review.\n",
    "\n",
    "The JSON object will look like this (shape, not exact values):\n",
    "\n",
    "{ddl_obj}\n",
    "\n",
    "Where:\n",
    "- table_name: name of the table\n",
    "- database: database / schema name\n",
    "- schema: list of columns, each like:\n",
    "  - name: column name\n",
    "  - type: data type (string, int, double, date, etc.)\n",
    "  - nullable: true / false\n",
    "  - partition_key: true / false\n",
    "  - primary_key: true / false\n",
    "  - foreign_key: true / false\n",
    "  - comments: free text with business meaning (e.g. \"cannot be null\", \"PII column\", \"FK : references employees_test(name)\")\n",
    "- column_stats: ONLY for NON-PII columns, something like:\n",
    "  - ROW_COUNT: \"1234\" (stringified)\n",
    "  - For each non-PII column:\n",
    "      - min, max, null_pct, distinct_count, top_values\n",
    "- constraints: optional list of primary keys, unique keys, check constraints (may be missing)\n",
    "- job_summary: includes inputs, filters, and grain if available\n",
    "- rule_type MUST be exactly one of the allowed values below.\n",
    "  Do NOT invent new rule_type names. If unsure, choose the closest one:\n",
    "  [not_null, not_empty, min, max, allowed_values, regex, pk, fk, check_constraint, custom_sql]\n",
    "\n",
    "IMPORTANT PRIVACY RULES:\n",
    "- PII columns (name, email, phone_number, salary, ssn, credit_card, and any column whose comment suggests PII) appear in \"schema\"\n",
    "  but their stats and values are NOT provided.\n",
    "- For PII columns:\n",
    "  - You MAY define structural rules: not_null, not_empty, length, regex.\n",
    "  - You MUST NOT include any concrete example values (no fake SSNs, emails, phones, names, etc.).\n",
    "  - Only describe patterns, like \"must be 9 digits\", \"must match email format\".\n",
    "- For non-PII columns:\n",
    "  - You MAY use column_stats to propose ranges and allowed_values.\n",
    "  - Still avoid writing specific sample values in descriptions; talk about rules, not sample data.\n",
    "\n",
    "----------------------------------------\n",
    "HOW TO USE COMMENTS\n",
    "----------------------------------------\n",
    "\n",
    "You MUST carefully read and interpret the \"comments\" field for each column. Comments can override or refine behaviour that is not obvious from the \"nullable\" flag or type alone.\n",
    "\n",
    "Examples:\n",
    "- If nullable = true but comments contain phrases like:\n",
    "  - \"cannot be null\", \"must not be null\", \"always present\", \"required field\"\n",
    "  Then you MUST treat this as a business rule: generate a not_null rule and a spark_exp that fails on NULL.\n",
    "\n",
    "- If comments describe business constraints (e.g. \"total number of employees present in this company\", \"must be positive\", \"status is one of ACTIVE, INACTIVE\"):\n",
    "  - You should translate this into appropriate rules: min/max, allowed_values, or custom_sql.\n",
    "\n",
    "- If comments describe relationships (e.g. \"FK : references employees_test(name)\"):\n",
    "  - You should generate an fk rule (and a suggested SQL test) that checks referential integrity at the table level.\n",
    "\n",
    "- If comments mention \"PII\", \"sensitive\", or \"confidential\":\n",
    "  - Treat the column as PII for purposes of privacy constraints.\n",
    "\n",
    "Comments are HIGH PRIORITY business knowledge and can override \"nullable\" if there is a conflict and any other relavant conditions. \n",
    "If nullable = true but comment says \"cannot be null\", you MUST enforce not_null in rules and spark_exp.\n",
    "\n",
    "----------------------------------------\n",
    "THINKING / COVERAGE REQUIREMENTS\n",
    "----------------------------------------\n",
    "\n",
    "You must think column-by-column and constraint-by-constraint.\n",
    "Do not skip any column.\n",
    "\n",
    "For coverage:\n",
    "- Every column in \"schema\" (except purely technical partition columns if clearly labeled as such) MUST appear in at least one rule in data_quality.rules.\n",
    "- Do NOT skip columns just because nullable = true.\n",
    "- Comments MUST be considered when deciding whether the column should be treated as logically required, even if nullable = true in the physical schema.\n",
    "\n",
    "For every column in \"schema\":\n",
    "\n",
    "1. COMPLETENESS\n",
    "\n",
    "   IMPORTANT:\n",
    "   - Normally:\n",
    "     - If nullable = true, you SHOULD NOT create a not_null rule.\n",
    "   - However, COMMENTS CAN OVERRIDE NULLABILITY:\n",
    "     - If comments clearly say \"cannot be null\", \"must not be null\", \"always present\", or similar:\n",
    "       -> You MUST treat this column as logically NOT NULL and generate a not_null rule and enforce non-null in spark_exp.\n",
    "   - Nullable columns must allow NULL in all spark_exp expressions unless comments explicitly say otherwise.\n",
    "   - Never generate contradictory rules for any column.\n",
    "     Examples of contradictions:\n",
    "       * nullable = true and comments are silent about nulls, but you generate not_null.\n",
    "       * comments say \"optional\" but you enforce not_null.\n",
    "\n",
    "   - If nullable = false OR comments indicate \"cannot be null\" → always generate a not_null rule.\n",
    "   - For ALL string-like columns (string, varchar, char), regardless of nullable:\n",
    "       * Always generate a not_empty-style rule:\n",
    "         - If column is logically nullable (nullable = true and comments do NOT forbid nulls):\n",
    "           spark_exp should be \"column IS NULL OR length(trim(column)) > 0\".\n",
    "         - If column is logically NOT NULL (nullable = false OR comments say cannot be null):\n",
    "           spark_exp should be \"column IS NOT NULL AND length(trim(column)) > 0\".\n",
    "\n",
    "2. VALIDITY\n",
    "\n",
    "   Use the combination of:\n",
    "   - column name\n",
    "   - data type\n",
    "   - comments\n",
    "   - column_stats (only non-PII)\n",
    "   - constraints (CHECK, PK, UNIQUE)\n",
    "   to infer validity rules such as:\n",
    "     * Numeric columns >= 0 unless obviously not applicable.\n",
    "     * head_count or similar metrics should be >= 0.\n",
    "     * String columns with stable lengths → infer min_length / max_length or regex.\n",
    "     * Year/date columns must not be in the future.\n",
    "     * Codes (country_code, department, status) with low cardinality should use allowed_values.\n",
    "     * Comments like \"there will be no company without a name\" → treat as logically not_null + not_empty.\n",
    "\n",
    "3. RANGE RULES (NON-PII ONLY)\n",
    "\n",
    "   Use column_stats[min, max, distinct_count, null_pct].\n",
    "   - Create soft WARNING rules with a 20–25% buffer around min/max or p95 if present.\n",
    "   - For example, if min = 1 and max = 287, you might allow [0.8 * min, 1.2 * max] as a WARNING range, if that makes sense for the domain.\n",
    "\n",
    "4. ALLOWED VALUES (NON-PII ONLY)\n",
    "\n",
    "   If distinct_count is small (< 50) AND stable → generate allowed_values.\n",
    "   - You should not list every value if there are too many; only when distinct_count is small.\n",
    "   - Do not echo literal values if they look like sensitive info; use them mainly when they are clearly categorical codes or domains.\n",
    "\n",
    "5. PII COLUMNS\n",
    "\n",
    "   - PII columns appear in schema but have NO column_stats.\n",
    "   - For these columns you MUST generate:\n",
    "       * not_null (if nullable = false or comments say cannot be null).\n",
    "       * not_empty for strings (with logic depending on nullability as above).\n",
    "       * regex or fixed-length patterns inferred ONLY from schema + column name + comments.\n",
    "   - NEVER include example email or SSN values. Only describe patterns.\n",
    "\n",
    "6. CROSS-COLUMN LOGIC (IF OBVIOUS)\n",
    "\n",
    "   Examples:\n",
    "   - If year/date columns exist → ensure year <= current_date.\n",
    "   - If comments suggest referential links (e.g. \"FK : references employees_test(name)\") → generate fk rules and a suggested SQL test that checks existence in the parent table.\n",
    "   - If joining_date and resign_date exist → resign_date >= joining_date.\n",
    "   - If head_count > 0 then company_name should not be null or empty, etc.\n",
    "\n",
    "7. TABLE-LEVEL RULES\n",
    "\n",
    "   - If constraints or comments imply a primary key → include uniqueness rule (rule_type = pk at __TABLE__ level).\n",
    "   - Add table-level rule: row_count > 0 (based on ROW_COUNT in column_stats if present).\n",
    "\n",
    "8. Data Quality (pre-load PySpark)\n",
    "\n",
    "   For each rule, you MUST output \"spark_exp\" using **Spark SQL syntax only**, not PySpark API.\n",
    "   spark_exp MUST be a SQL expression that can be passed directly into:\n",
    "\n",
    "       df.filter(expr(spark_exp))\n",
    "\n",
    "   Examples of valid spark_exp:\n",
    "     - \"salary >= 0\"\n",
    "     - \"salary IS NULL OR salary >= 0\"\n",
    "     - \"name IS NULL OR length(trim(name)) > 0\"\n",
    "     - \"joining_date IS NULL OR joining_date <= current_date()\"\n",
    "     - \"head_count IS NULL OR head_count >= 0\"\n",
    "\n",
    "   Examples of INVALID spark_exp (do NOT generate these):\n",
    "     - col('salary') >= 0\n",
    "     - F.col(\"name\").isNull()\n",
    "     - dataframe.count() > 0\n",
    "     - salary.notNull()\n",
    "     - any reference to a DataFrame variable\n",
    "\n",
    "   The semantic convention is:\n",
    "   - spark_exp should evaluate to TRUE for valid rows.\n",
    "   - The pipeline will typically use NOT (spark_exp) to find invalid rows.\n",
    "\n",
    "9. TEST COVERAGE (Post-load SQL)\n",
    "\n",
    "   You must generate SQL tests for:\n",
    "       * uniqueness of PK/grain (if applicable).\n",
    "       * null checks on required columns (including where comments indicate \"cannot be null\").\n",
    "       * each CHECK-style constraint or strong business rule you infer.\n",
    "       * future-date violations.\n",
    "       * allowed_values validation (for low-cardinality columns).\n",
    "       * numeric range violations.\n",
    "\n",
    "After generating rules and tests, REVIEW:\n",
    "- Did you include ALL columns?\n",
    "- Did you cover ALL non-nullable and logically non-null columns (including comment-based ones)?\n",
    "- Did you enforce ALL constraints and clear business rules from comments?\n",
    "- Did you create BOTH rules AND tests?\n",
    "\n",
    "----------------------------------------\n",
    "SUGGESTIONS SECTION (OPTIONAL / OUT-OF-THE-BOX)\n",
    "----------------------------------------\n",
    "\n",
    "After you finish generating \"data_quality\", \"tests\", and \"docs_markdown\" (strictly grounded in the input JSON and the rules above), you MUST generate a separate section called \"suggestions\".\n",
    "\n",
    "Rules for \"suggestions\":\n",
    "- \"suggestions\" is ONLY for human review. The pipeline does NOT automatically enforce anything from this section.\n",
    "- In \"suggestions\" you MAY go beyond the explicit metadata and comments, as long as:\n",
    "  - You do NOT contradict the schema, data types, or explicit comments.\n",
    "  - You stay within realistic, common data-engineering best practices.\n",
    "- Good examples of suggestions:\n",
    "  - Extra data quality rules that are common in real systems (e.g. latency checks, distribution checks).\n",
    "  - Additional business-rule candidates that a human could consider.\n",
    "  - PII / governance hints (e.g. require masking in downstream layers).\n",
    "  - Suggestions for more detailed allowed_values or cross-table checks.\n",
    "\n",
    "Each suggestion MUST include:\n",
    "- A high-level \"description\" of the suggestion.\n",
    "- \"type\": must be exactly \"SUGGESTION_ONLY\".\n",
    "- \"area\": one of [business_rule, dq_rule, pii, governance, performance, schema_design].\n",
    "- A \"rule\" object with the SAME structure as a normal rule, plus an optional \"sql_test\":\n",
    "  - column, rule_type, condition, severity, action, description, spark_exp, sql_test, notes.\n",
    "\n",
    "\"data_quality\" and \"tests\" MUST remain strictly grounded in the actual JSON and comments.\n",
    "All speculative or optional ideas MUST go only into \"suggestions\".\n",
    "\n",
    "----------------------------------------\n",
    "OUTPUT FORMAT (MUST BE VALID JSON)\n",
    "----------------------------------------\n",
    "\n",
    "Important:\n",
    "- In the final JSON, the set of column names used in data_quality.rules (excluding \"__TABLE__\") MUST match the set of column names in \"schema\" (case-insensitive). Do not omit any columns.\n",
    "- Coverage requirement does NOT override the schema and comments.\n",
    "- If a column is nullable, you may generate \"if present, must...\" rules (e.g., not_empty with NULL allowed).\n",
    "- If comments explicitly indicate \"cannot be null\" or equivalent, you MUST treat the column as logically non-null and enforce not_null in both rules and spark_exp.\n",
    "\n",
    "Return ONLY valid JSON in this exact structure (no extra comments):\n",
    "\n",
    "{\n",
    "  \"data_quality\": {\n",
    "    \"rules\": [\n",
    "      {\n",
    "        \"column\": \"col_name_or__TABLE__for_table_level\",\n",
    "        \"rule_type\": \"not_null | not_empty | min | max | allowed_values | regex | pk | fk | check_constraint | custom_sql\",\n",
    "        \"condition\": \"value / list / SQL expression / description string\",\n",
    "        \"severity\": \"ERROR | WARNING\",\n",
    "        \"action\": \"FAIL_JOB | DROP_ROW | WARN\",\n",
    "        \"description\": \"Short reasoning for the rule (no concrete example values).\",\n",
    "        \"spark_exp\": \"Spark SQL boolean expression that returns TRUE for valid rows and can be passed directly to pyspark.sql.functions.expr(). It MUST NOT reference any DataFrame variable and MUST NOT call actions like count(), groupBy(), collect(), etc. Examples: \\\"salary IS NULL OR salary >= 0\\\", \\\"name IS NULL OR length(trim(name)) > 0\\\", \\\"joining_date IS NULL OR joining_date <= current_date()\\\".\"\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  \"tests\": [\n",
    "    {\n",
    "      \"name\": \"test_name\",\n",
    "      \"sql\": \"SELECT ...\",\n",
    "      \"description\": \"What this test validates.\"\n",
    "    }\n",
    "  ],\n",
    "  \"docs_markdown\": \"# Table Documentation\\n...\",\n",
    "  \"suggestions\": [\n",
    "    {\n",
    "      \"area\": \"business_rule | dq_rule | pii | governance | performance | schema_design\",\n",
    "      \"type\": \"SUGGESTION_ONLY\",\n",
    "      \"description\": \"Human-readable suggestion. Do not treat this as enforced logic.\",\n",
    "      \"rule\": {\n",
    "        \"column\": \"col_name_or__TABLE__for_table_level\",\n",
    "        \"rule_type\": \"not_null | not_empty | min | max | allowed_values | regex | pk | fk | check_constraint | custom_sql\",\n",
    "        \"condition\": \"value / list / SQL expression / description string\",\n",
    "        \"severity\": \"ERROR | WARNING\",\n",
    "        \"action\": \"FAIL_JOB | DROP_ROW | WARN\",\n",
    "        \"description\": \"Reasoning for this suggested rule (no concrete sample values).\",\n",
    "        \"spark_exp\": \"Spark SQL boolean expression, same constraints as data_quality.rules.spark_exp.\",\n",
    "        \"sql_test\": \"SELECT ...\",\n",
    "        \"notes\": \"Optional notes. Only apply this after human review.\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "Do NOT include anything outside this JSON object.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "013e0a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = payload.replace(\"{ddl_obj}\", json.dumps(ddl_obj, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6384c1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"data_quality\": {\n",
      "    \"rules\": [\n",
      "      {\n",
      "        \"column\": \"__TABLE__\",\n",
      "        \"rule_type\": \"pk\",\n",
      "        \"condition\": \"emp_id\",\n",
      "        \"severity\": \"ERROR\",\n",
      "        \"action\": \"FAIL_JOB\",\n",
      "        \"description\": \"The 'emp_id' column must be unique across all rows. Column stats indicate there are currently duplicate values, which is a critical quality issue.\",\n",
      "        \"spark_exp\": \"null\"\n",
      "      },\n",
      "      {\n",
      "        \"column\": \"__TABLE__\",\n",
      "        \"rule_type\": \"custom_sql\",\n",
      "        \"condition\": \"row_count > 0\",\n",
      "        \"severity\": \"ERROR\",\n",
      "        \"action\": \"FAIL_JOB\",\n",
      "        \"description\": \"The table must contain at least one row to be considered valid for processing.\",\n",
      "        \"spark_exp\": \"null\"\n",
      "      },\n",
      "      {\n",
      "        \"column\": \"emp_id\",\n",
      "        \"rule_type\": \"not_null\",\n",
      "        \"condition\": null,\n",
      "        \"severity\": \"ERROR\",\n",
      "        \"action\": \"FAIL_JOB\",\n",
      "        \"description\": \"Employee ID is a required identifier and cannot be null, based on business context and observed 0% nulls in column stats.\",\n",
      "        \"spark_exp\": \"emp_id IS NOT NULL\"\n",
      "      },\n",
      "      {\n",
      "        \"column\": \"emp_id\",\n",
      "        \"rule_type\": \"min\",\n",
      "        \"condition\": \"1\",\n",
      "        \"severity\": \"ERROR\",\n",
      "        \"action\": \"FAIL_JOB\",\n",
      "        \"description\": \"Employee ID must be a positive integer.\",\n",
      "        \"spark_exp\": \"emp_id > 0\"\n",
      "      },\n",
      "      {\n",
      "        \"column\": \"name\",\n",
      "        \"rule_type\": \"not_empty\",\n",
      "        \"condition\": null,\n",
      "        \"severity\": \"ERROR\",\n",
      "        \"action\": \"DROP_ROW\",\n",
      "        \"description\": \"If a name is provided, it cannot be an empty or whitespace-only string. Null values are allowed as per schema.\",\n",
      "        \"spark_exp\": \"name IS NULL OR length(trim(name)) > 0\"\n",
      "      },\n",
      "      {\n",
      "        \"column\": \"salary\",\n",
      "        \"rule_type\": \"min\",\n",
      "        \"condition\": \"0\",\n",
      "        \"severity\": \"ERROR\",\n",
      "        \"action\": \"DROP_ROW\",\n",
      "        \"description\": \"If a salary is provided, it must be a non-negative value. Null values are allowed as per schema.\",\n",
      "        \"spark_exp\": \"salary IS NULL OR salary >= 0\"\n",
      "      },\n",
      "      {\n",
      "        \"column\": \"department\",\n",
      "        \"rule_type\": \"not_empty\",\n",
      "        \"condition\": null,\n",
      "        \"severity\": \"ERROR\",\n",
      "        \"action\": \"DROP_ROW\",\n",
      "        \"description\": \"If a department is provided, it cannot be an empty or whitespace-only string. Null values are allowed as per schema.\",\n",
      "        \"spark_exp\": \"department IS NULL OR length(trim(department)) > 0\"\n",
      "      },\n",
      "      {\n",
      "        \"column\": \"joining_date\",\n",
      "        \"rule_type\": \"not_null\",\n",
      "        \"condition\": null,\n",
      "        \"severity\": \"ERROR\",\n",
      "        \"action\": \"FAIL_JOB\",\n",
      "        \"description\": \"Employee joining date is a required field and cannot be null, based on business context and observed 0% nulls in column stats.\",\n",
      "        \"spark_exp\": \"joining_date IS NOT NULL\"\n",
      "      },\n",
      "      {\n",
      "        \"column\": \"joining_date\",\n",
      "        \"rule_type\": \"max\",\n",
      "        \"condition\": \"current_date()\",\n",
      "        \"severity\": \"ERROR\",\n",
      "        \"action\": \"DROP_ROW\",\n",
      "        \"description\": \"Employee joining date cannot be in the future.\",\n",
      "        \"spark_exp\": \"joining_date <= current_date()\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"tests\": [\n",
      "    {\n",
      "      \"name\": \"test_uniqueness_emp_id\",\n",
      "      \"sql\": \"SELECT emp_id, COUNT(*) AS duplicate_count FROM employees_test GROUP BY emp_id HAVING COUNT(*) > 1\",\n",
      "      \"description\": \"This test should return 0 rows. Any rows returned indicate duplicate 'emp_id' values, violating the primary key constraint. Based on current stats, this test is expected to fail.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"test_not_null_emp_id\",\n",
      "      \"sql\": \"SELECT COUNT(1) FROM employees_test WHERE emp_id IS NULL\",\n",
      "      \"description\": \"This test should return 0. Any count greater than 0 indicates null values in the 'emp_id' column.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"test_positive_emp_id\",\n",
      "      \"sql\": \"SELECT COUNT(1) FROM employees_test WHERE emp_id <= 0\",\n",
      "      \"description\": \"This test should return 0. Any count greater than 0 indicates non-positive values in the 'emp_id' column.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"test_not_empty_name\",\n",
      "      \"sql\": \"SELECT COUNT(1) FROM employees_test WHERE name IS NOT NULL AND length(trim(name)) = 0\",\n",
      "      \"description\": \"This test should return 0. Any count greater than 0 indicates that non-null 'name' values are empty strings.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"test_non_negative_salary\",\n",
      "      \"sql\": \"SELECT COUNT(1) FROM employees_test WHERE salary IS NOT NULL AND salary < 0\",\n",
      "      \"description\": \"This test should return 0. Any count greater than 0 indicates negative salary values.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"test_not_null_joining_date\",\n",
      "      \"sql\": \"SELECT COUNT(1) FROM employees_test WHERE joining_date IS NULL\",\n",
      "      \"description\": \"This test should return 0. Any count greater than 0 indicates null values in the 'joining_date' column.\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"test_future_joining_date\",\n",
      "      \"sql\": \"SELECT COUNT(1) FROM employees_test WHERE joining_date > current_date\",\n",
      "      \"description\": \"This test should return 0. Any count greater than 0 indicates joining dates are set in the future.\"\n",
      "    }\n",
      "  ],\n",
      "  \"docs_markdown\": \"# Table: employees_test\\n\\n**Database:** `copilot_demo`\\n\\n## Description\\nThis table contains information about employees, including their ID, name, salary, department, and joining date.\\n\\n## Column Details\\n\\n| Column Name    | Data Type | Nullable | Partition Key | Description                                                                 |\\n|----------------|-----------|----------|---------------|-----------------------------------------------------------------------------|\\n| `emp_id`       | int       | Yes      | No            | The unique identifier for an employee. Business rules require this to be non-null and unique. |\\n| `name`         | string    | Yes      | No            | The full name of the employee. **(PII)**                                    |\\n| `salary`       | double    | Yes      | No            | The employee's salary. **(PII)**                                            |\\n| `department`   | string    | Yes      | No            | The department the employee belongs to.                                     |\\n| `joining_date` | date      | Yes      | No            | The date the employee joined the company. Business rules require this to be non-null. |\\n\\n## Data Quality Summary\\n- **Primary Key:** `emp_id` is the designated primary key, but data profiling indicates existing duplicate values which must be remediated.\\n- **Required Fields:** `emp_id` and `joining_date` are considered business-critical and must not be null.\\n- **PII:** The `name` and `salary` columns contain Personally Identifiable Information and must be handled with appropriate security and governance controls.\\n\\n## File and Update Information\\n- **File Format:** unknown\\n- **Update Frequency:** unknown\\n- **S3 Location:** unknown\",\n",
      "  \"suggestions\": [\n",
      "    {\n",
      "      \"area\": \"dq_rule\",\n",
      "      \"type\": \"SUGGESTION_ONLY\",\n",
      "      \"description\": \"Define a fixed list of allowed departments to enforce consistency. The table currently has 6 distinct departments.\",\n",
      "      \"rule\": {\n",
      "        \"column\": \"department\",\n",
      "        \"rule_type\": \"allowed_values\",\n",
      "        \"condition\": \"['Finance', 'HR', 'IT', 'Marketing', 'Sales', 'Support']\",\n",
      "        \"severity\": \"ERROR\",\n",
      "        \"action\": \"DROP_ROW\",\n",
      "        \"description\": \"The department value must be one of the officially recognized departments.\",\n",
      "        \"spark_exp\": \"department IS NULL OR department IN ('Finance', 'HR', 'IT', 'Marketing', 'Sales', 'Support')\",\n",
      "        \"sql_test\": \"SELECT DISTINCT department FROM employees_test WHERE department NOT IN ('Finance', 'HR', 'IT', 'Marketing', 'Sales', 'Support')\",\n",
      "        \"notes\": \"The list of departments is inferred and requires confirmation from a business stakeholder before enforcement.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"area\": \"business_rule\",\n",
      "      \"type\": \"SUGGESTION_ONLY\",\n",
      "      \"description\": \"Consider making the 'name' column non-nullable, as every employee record should have a name.\",\n",
      "      \"rule\": {\n",
      "        \"column\": \"name\",\n",
      "        \"rule_type\": \"not_null\",\n",
      "        \"condition\": null,\n",
      "        \"severity\": \"ERROR\",\n",
      "        \"action\": \"FAIL_JOB\",\n",
      "        \"description\": \"Employee name cannot be null.\",\n",
      "        \"spark_exp\": \"name IS NOT NULL\",\n",
      "        \"sql_test\": \"SELECT COUNT(1) FROM employees_test WHERE name IS NULL\",\n",
      "        \"notes\": \"This enforces a stricter business rule than the current schema. Please confirm with data owners.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"area\": \"schema_design\",\n",
      "      \"type\": \"SUGGESTION_ONLY\",\n",
      "      \"description\": \"The schema for 'emp_id' and 'joining_date' allows NULLs, but business context and data stats imply they are required. Consider altering the table DDL to set these columns as NOT NULL.\",\n",
      "      \"rule\": {\n",
      "        \"column\": \"emp_id, joining_date\",\n",
      "        \"rule_type\": \"not_null\",\n",
      "        \"condition\": \"DDL change to NOT NULL\",\n",
      "        \"severity\": \"ERROR\",\n",
      "        \"action\": \"FAIL_JOB\",\n",
      "        \"description\": \"Core identifier and date columns should be non-nullable at the schema level.\",\n",
      "        \"spark_exp\": \"emp_id IS NOT NULL AND joining_date IS NOT NULL\",\n",
      "        \"sql_test\": null,\n",
      "        \"notes\": \"This change would prevent nullable data from being written in the first place, improving data integrity at the source.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"area\": \"pii\",\n",
      "      \"type\": \"SUGGESTION_ONLY\",\n",
      "      \"description\": \"Establish governance rules for PII columns 'name' and 'salary', such as masking or access control policies in downstream systems.\",\n",
      "      \"rule\": {\n",
      "        \"column\": \"name, salary\",\n",
      "        \"rule_type\": \"custom_sql\",\n",
      "        \"condition\": \"Apply masking or access controls\",\n",
      "        \"severity\": \"WARNING\",\n",
      "        \"action\": \"WARN\",\n",
      "        \"description\": \"PII data must be protected.\",\n",
      "        \"spark_exp\": \"true\",\n",
      "        \"sql_test\": null,\n",
      "        \"notes\": \"This is a governance suggestion, not a data validation rule. It should be implemented via AWS Lake Formation, IAM policies, or view-level security.\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"area\": \"dq_rule\",\n",
      "      \"type\": \"SUGGESTION_ONLY\",\n",
      "      \"description\": \"Monitor for unexpected changes in row count between pipeline runs, which could indicate upstream data loading issues.\",\n",
      "      \"rule\": {\n",
      "        \"column\": \"__TABLE__\",\n",
      "        \"rule_type\": \"custom_sql\",\n",
      "        \"condition\": \"Row count change is within 25% of the previous run's count.\",\n",
      "        \"severity\": \"WARNING\",\n",
      "        \"action\": \"WARN\",\n",
      "        \"description\": \"Detects abnormal drops or spikes in data volume.\",\n",
      "        \"spark_exp\": \"null\",\n",
      "        \"sql_test\": null,\n",
      "        \"notes\": \"This requires state management to compare row counts between runs. The percentage threshold should be tuned based on expected volatility.\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "client = genai.Client()\n",
    "response = client.models.generate_content(model='gemini-2.5-pro', contents=payload)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602e6ddc",
   "metadata": {},
   "source": [
    "# Push to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5624404",
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_json_path = \"s3://de-copilot-s3/contracts/\"\n",
    "s3_client = boto3.client('s3',region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e7abda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_json_str = re.sub(r\"```json\\n|\\n```\", \"\", response.text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5b1ab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_json = json.loads(clean_json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32141ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'MAY7KSGK88MB18RK',\n",
       "  'HostId': 'oZ+X5mZ309qKCKN8nAE6DDziYBla5fBjNq0L/boUIj0EyUuNIqx862uw2uhEkh55VtH6PudYUH4fhR83r/D9t/A1S9BoGXZv',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'oZ+X5mZ309qKCKN8nAE6DDziYBla5fBjNq0L/boUIj0EyUuNIqx862uw2uhEkh55VtH6PudYUH4fhR83r/D9t/A1S9BoGXZv',\n",
       "   'x-amz-request-id': 'MAY7KSGK88MB18RK',\n",
       "   'date': 'Sun, 23 Nov 2025 18:58:33 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"b185c99ce1de48a934ecb2165038253f\"',\n",
       "   'x-amz-checksum-crc32': '8+l7aA==',\n",
       "   'x-amz-checksum-type': 'FULL_OBJECT',\n",
       "   'content-length': '0',\n",
       "   'server': 'AmazonS3'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"b185c99ce1de48a934ecb2165038253f\"',\n",
       " 'ChecksumCRC32': '8+l7aA==',\n",
       " 'ChecksumType': 'FULL_OBJECT',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_client.put_object(Bucket='de-copilot-s3',Key=f'contracts/{table}.json',Body=json.dumps(cleaned_json,indent=2),ContentType='application/json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9176f2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'table_name': 'employees_test',\n",
       " 'database': 'copilot_demo',\n",
       " 'schema': [{'name': 'emp_id',\n",
       "   'type': 'int',\n",
       "   'nullable': False,\n",
       "   'partition_key': False,\n",
       "   'primary_key': True,\n",
       "   'foregin_key': False,\n",
       "   'comments': 'primary_key'},\n",
       "  {'name': 'name',\n",
       "   'type': 'string',\n",
       "   'nullable': True,\n",
       "   'partition_key': False,\n",
       "   'primary_key': False,\n",
       "   'foregin_key': False,\n",
       "   'comments': ''},\n",
       "  {'name': 'salary',\n",
       "   'type': 'double',\n",
       "   'nullable': True,\n",
       "   'partition_key': False,\n",
       "   'primary_key': False,\n",
       "   'foregin_key': False,\n",
       "   'comments': ''},\n",
       "  {'name': 'department',\n",
       "   'type': 'string',\n",
       "   'nullable': True,\n",
       "   'partition_key': False,\n",
       "   'primary_key': False,\n",
       "   'foregin_key': False,\n",
       "   'comments': ''},\n",
       "  {'name': 'joining_date',\n",
       "   'type': 'date',\n",
       "   'nullable': True,\n",
       "   'partition_key': False,\n",
       "   'primary_key': False,\n",
       "   'foregin_key': False,\n",
       "   'comments': 'cannot be null'}],\n",
       " 'column_stats': {'ROW_COUNT': '261',\n",
       "  'joining_date': {'min': '2011-01-17',\n",
       "   'max': '2023-11-30',\n",
       "   'null_pct': 0.0038314176245210726,\n",
       "   'distinct_count': 250},\n",
       "  'department': {'min': 'Finance',\n",
       "   'max': 'Support',\n",
       "   'null_pct': 0.0,\n",
       "   'distinct_count': 6},\n",
       "  'emp_id': {'min': '1',\n",
       "   'max': '287',\n",
       "   'null_pct': 0.0,\n",
       "   'distinct_count': 258}}}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddl_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42563993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
